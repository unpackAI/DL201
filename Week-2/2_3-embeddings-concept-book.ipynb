{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Week 2: Embeddings Concept Book","metadata":{}},{"cell_type":"markdown","source":"# Introduction\n<hr style=\"border:2px solid gray\"> </hr>\n","metadata":{}},{"cell_type":"markdown","source":"In the previous notebook, we learned about categorical variables and the challenge to put them into AI models. Here, we work with the human languages, made of dozens of thousands of different tokens. This problem seems insurmountable, because we are no longer dealing with a small amount of categories anymore.\n\nHere, as we did with the latent factors in the recommender systems, we represent each word as a large vector. Not only does this solve the problem of encoding, but also this is creating very meaningful coordinates, enable to represent the similarities between words, finding homonyms, synonyms, and much more !","metadata":{}},{"cell_type":"markdown","source":"### Goals and Objectives\n<hr style=\"border:2px solid gray\"> </hr>\n","metadata":{}},{"cell_type":"markdown","source":"* Have an appreciation on the way words can be represented as vectors \n* Grasp the concept behind the distribution hypothesis\n* See the potential of embeddings","metadata":{}},{"cell_type":"markdown","source":"### Key Ideas\n<hr style=\"border:2px solid gray\"> </hr>\n\n* Vector Encoding\n* Feature Space\n* Distribution Hypothesis\n* Dimensionality Reduction","metadata":{}},{"cell_type":"code","source":"is_kaggle = True   # True if you are on Kaggle, False for local Windows, Linux or Mac environments.","metadata":{"execution":{"iopub.status.busy":"2022-07-20T14:14:40.726682Z","iopub.execute_input":"2022-07-20T14:14:40.727146Z","iopub.status.idle":"2022-07-20T14:14:40.761233Z","shell.execute_reply.started":"2022-07-20T14:14:40.727043Z","shell.execute_reply":"2022-07-20T14:14:40.760272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# libraries installation\nif is_kaggle:\n    !pip install -Uqqq spaCy \n    !python -m spacy download en_core_web_lg\n    from IPython.display import clear_output\n    clear_output()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-07-20T14:14:40.762874Z","iopub.execute_input":"2022-07-20T14:14:40.763217Z","iopub.status.idle":"2022-07-20T14:17:08.057479Z","shell.execute_reply.started":"2022-07-20T14:14:40.763163Z","shell.execute_reply":"2022-07-20T14:17:08.056081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import spatial # to compute the distance between words vectors\nimport spacy # to load a word embedding","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-07-20T14:17:08.060119Z","iopub.execute_input":"2022-07-20T14:17:08.060586Z","iopub.status.idle":"2022-07-20T14:17:12.519716Z","shell.execute_reply.started":"2022-07-20T14:17:08.06053Z","shell.execute_reply":"2022-07-20T14:17:12.518654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load('en_core_web_lg')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-07-20T14:17:12.521058Z","iopub.execute_input":"2022-07-20T14:17:12.5225Z","iopub.status.idle":"2022-07-20T14:17:14.827434Z","shell.execute_reply.started":"2022-07-20T14:17:12.522436Z","shell.execute_reply":"2022-07-20T14:17:14.825781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The Distribution Hypothesis\n<hr style=\"border:2px solid gray\"> </hr>\n\nWord embeddings are built on top of something called the <em>Distribution Hypothesis</em>.\n\nThe Distribution Hypothesis, in plain English, is the idea that in large bodies of text, certain words will probably appear more closely to eachother than than words that are unrelated.\n\nEmbeddings can be trained using data, such as from Wikipedia, news articles, or questions and answers, and achieve some shocking results. \n\nThe distance between these vectors can be calculated by using cosine similarity.","metadata":{}},{"cell_type":"markdown","source":"Let's say that we have A1, A2, B1 and B2.\nThere is almost the same relationship between A1 and A2, and between B1 and B2 (for instance, between man and woman, and between king and queen).\n\nThen, we have \nA1 - A2 = B1 - B2\n\nSo, an approximation of B2 is equal to\n\nB2' = B1 - (A1 - A2)\n\nB2' = B1 - A1 + A2","metadata":{}},{"cell_type":"code","source":"cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-07-20T14:17:14.83018Z","iopub.execute_input":"2022-07-20T14:17:14.830978Z","iopub.status.idle":"2022-07-20T14:17:15.063667Z","shell.execute_reply.started":"2022-07-20T14:17:14.830925Z","shell.execute_reply":"2022-07-20T14:17:15.062587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"A1 = nlp.vocab['female'].vector\nA2 = nlp.vocab['male'].vector\n\nB1 = nlp.vocab['queen'].vector\n\n# We now need to find the closest vector in the vocabulary to the result of \"man\" - \"woman\" + \"queen\"\nB2_approximation = B1 - A1 + A2\n#approximation of king = queen - female + male\n\ncomputed_similarities = []\n \nfor word in nlp.vocab:\n    # Ignore words without vectors\n    if not word.has_vector:\n        continue\n \n    similarity = cosine_similarity(B2_approximation, word.vector)\n    computed_similarities.append((word, similarity))\n \ncomputed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\nprint('Closest words in the vector space: ')\nprint([w[0].text for w in computed_similarities[:9]])","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-07-20T14:17:15.065302Z","iopub.execute_input":"2022-07-20T14:17:15.065759Z","iopub.status.idle":"2022-07-20T14:17:15.115136Z","shell.execute_reply.started":"2022-07-20T14:17:15.065711Z","shell.execute_reply":"2022-07-20T14:17:15.113887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, are the most similar words to the vector. It works suprisingly well, and even includes slang. Cuz is short for Cousin, and is an informal way to address a friend, such as calling someone dude or bro.","metadata":{}},{"cell_type":"markdown","source":"### Dimensionality Reduction\n<hr style=\"border:2px solid gray\"> </hr>\n","metadata":{"execution":{"iopub.execute_input":"2022-03-08T09:15:27.187663Z","iopub.status.busy":"2022-03-08T09:15:27.187304Z","iopub.status.idle":"2022-03-08T09:15:27.194075Z","shell.execute_reply":"2022-03-08T09:15:27.192999Z","shell.execute_reply.started":"2022-03-08T09:15:27.187625Z"}}},{"cell_type":"markdown","source":"If we go back to the encoding examples, then it shows us how different representation can expand the feature space tremendously. Embeddings, once trained are suprisingly compared to a one hot dictionary encoding. ","metadata":{}},{"cell_type":"code","source":"A1.shape","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-07-20T14:18:19.650394Z","iopub.execute_input":"2022-07-20T14:18:19.650841Z","iopub.status.idle":"2022-07-20T14:18:19.659939Z","shell.execute_reply.started":"2022-07-20T14:18:19.650808Z","shell.execute_reply":"2022-07-20T14:18:19.659142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The embedding for a word, in this model, is embodied in a one dimension vector, of size 300.\nBut we can also consider this as a point in a 300 D space (it would have 300 coordinates). And we can use Principal Component Analysis to represent it in 2 or 3D space.","metadata":{}}]}