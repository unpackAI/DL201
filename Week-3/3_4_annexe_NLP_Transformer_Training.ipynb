{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/philanoe/nlp-transformer-training?scriptVersionId=102105898\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"## ðŸ’» UnpackAI DL201 Bootcamp - Week 3 - Training a NLP transformer\n\n### ðŸ“• Learning Objectives\n\n* Getting working examples able to achieve the main NLP tasks\n* Knowing the existence of Hugging Face and the strenth of its pre-trained models and all-in-one pipelines\n\n### ðŸ“– Concepts map\n\n* Pipeline\n* Training","metadata":{}},{"cell_type":"markdown","source":"This code was tested on Kaggle, without Accelerator, on 2022/7/30.","metadata":{}},{"cell_type":"code","source":"# install the necessary libraries (need internet access)\n!pip install -Uqq datasets # transformers libraries\n!pip install -Uqq wandb # used during transformer training, even if we disable it","metadata":{"execution":{"iopub.status.busy":"2022-07-30T09:58:33.495797Z","iopub.execute_input":"2022-07-30T09:58:33.496907Z","iopub.status.idle":"2022-07-30T09:58:53.932925Z","shell.execute_reply.started":"2022-07-30T09:58:33.496775Z","shell.execute_reply":"2022-07-30T09:58:53.931018Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# libraries importation\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json\nimport datasets #library used to adapt our data to the transformers\n# the following libraries are here to download the pre-trained model, tokenize the training data, define the training arguments...\nfrom transformers import DataCollatorWithPadding, AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification\n\nimport os\nfrom pathlib import Path\nfor dirname, _, filenames in os.walk(\"../input/intent-recognition-chatbot-corpus-from-askubuntu\"):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-30T09:58:53.935154Z","iopub.execute_input":"2022-07-30T09:58:53.935605Z","iopub.status.idle":"2022-07-30T09:58:58.777562Z","shell.execute_reply.started":"2022-07-30T09:58:53.935568Z","shell.execute_reply":"2022-07-30T09:58:58.776069Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"../input/intent-recognition-chatbot-corpus-from-askubuntu/AskUbuntu Corpus.json\n","output_type":"stream"}]},{"cell_type":"code","source":"# environment preparation\nos.environ[\"TOKENIZERS_PARALLELISM\"]=\"true\"\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n#Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5.\n# Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).","metadata":{"execution":{"iopub.status.busy":"2022-07-30T09:58:58.77984Z","iopub.execute_input":"2022-07-30T09:58:58.781369Z","iopub.status.idle":"2022-07-30T09:58:58.787181Z","shell.execute_reply.started":"2022-07-30T09:58:58.781316Z","shell.execute_reply":"2022-07-30T09:58:58.785859Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#check if the environment variable was set correctly\nprint(os.environ.get('TOKENIZERS_PARALLELISM', ''))\nprint(os.environ.get('WANDB_DISABLED', ''))","metadata":{"execution":{"iopub.status.busy":"2022-07-30T09:58:58.790981Z","iopub.execute_input":"2022-07-30T09:58:58.791504Z","iopub.status.idle":"2022-07-30T09:58:58.805711Z","shell.execute_reply.started":"2022-07-30T09:58:58.79146Z","shell.execute_reply":"2022-07-30T09:58:58.804273Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"true\ntrue\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# First part : train a sentence classifier with the entire available data","metadata":{}},{"cell_type":"code","source":"# read json data as a dictionary \nwith open('../input/intent-recognition-chatbot-corpus-from-askubuntu/AskUbuntu Corpus.json', 'r') as f:\n  data = json.load(f)\n# Intent and Text information are stored in the value corresponding to sentences key \nsentences=data[\"sentences\"]\n# Get intent content using list comprehension by looping in the sentences values \nlabelList=[i[\"intent\"]for i in sentences]\n# Get text content using list comprehension by looping in the sentences values \ntextList=[i['text'] for i in sentences]","metadata":{"execution":{"iopub.status.busy":"2022-07-30T09:58:58.8073Z","iopub.execute_input":"2022-07-30T09:58:58.807854Z","iopub.status.idle":"2022-07-30T09:58:58.829337Z","shell.execute_reply.started":"2022-07-30T09:58:58.807811Z","shell.execute_reply":"2022-07-30T09:58:58.828295Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Create IntentDataFrame with label list and text list\nDFData = {'label' : labelList, 'sentence' : textList}\nIntentDataFrame = pd.DataFrame(data = DFData)","metadata":{"execution":{"iopub.status.busy":"2022-07-30T09:58:58.831003Z","iopub.execute_input":"2022-07-30T09:58:58.831629Z","iopub.status.idle":"2022-07-30T09:58:58.83997Z","shell.execute_reply.started":"2022-07-30T09:58:58.83159Z","shell.execute_reply":"2022-07-30T09:58:58.838739Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Delete the samples with \"None\" as label\nIntentDataFrame=IntentDataFrame[IntentDataFrame[\"label\"]!=\"None\"]","metadata":{"execution":{"iopub.status.busy":"2022-07-30T09:58:58.841856Z","iopub.execute_input":"2022-07-30T09:58:58.842899Z","iopub.status.idle":"2022-07-30T09:58:58.861027Z","shell.execute_reply.started":"2022-07-30T09:58:58.842851Z","shell.execute_reply":"2022-07-30T09:58:58.860014Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# check whether the training values are quite balanced\nIntentDataFrame[\"label\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-07-30T09:58:58.862619Z","iopub.execute_input":"2022-07-30T09:58:58.863533Z","iopub.status.idle":"2022-07-30T09:58:58.880365Z","shell.execute_reply.started":"2022-07-30T09:58:58.863493Z","shell.execute_reply":"2022-07-30T09:58:58.879025Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Software Recommendation    57\nMake Update                47\nShutdown Computer          27\nSetup Printer              23\nName: label, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# replace the labels strings by label numbers (we could automate this process for larger labels sets)\nLabelToIndex = {\"Software Recommendation\":0,\"Make Update\":1,\"Shutdown Computer\":2,\"Setup Printer\":3}\nIntentDataFrame[\"label\"]=IntentDataFrame[\"label\"].map(LabelToIndex)","metadata":{"execution":{"iopub.status.busy":"2022-07-30T09:58:58.88202Z","iopub.execute_input":"2022-07-30T09:58:58.882826Z","iopub.status.idle":"2022-07-30T09:58:58.895892Z","shell.execute_reply.started":"2022-07-30T09:58:58.882765Z","shell.execute_reply":"2022-07-30T09:58:58.894568Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# convert train_df to a dataset so that it can be used by Hugging Face models and tokenizers\ntrain_dataset=datasets.Dataset.from_pandas(IntentDataFrame)","metadata":{"execution":{"iopub.status.busy":"2022-07-30T09:58:58.901324Z","iopub.execute_input":"2022-07-30T09:58:58.902286Z","iopub.status.idle":"2022-07-30T09:58:58.918313Z","shell.execute_reply.started":"2022-07-30T09:58:58.902241Z","shell.execute_reply":"2022-07-30T09:58:58.917173Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-07-30T09:58:58.920008Z","iopub.execute_input":"2022-07-30T09:58:58.920686Z","iopub.status.idle":"2022-07-30T09:58:58.929561Z","shell.execute_reply.started":"2022-07-30T09:58:58.92065Z","shell.execute_reply":"2022-07-30T09:58:58.927898Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['label', 'sentence', '__index_level_0__'],\n    num_rows: 154\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"# Remove __index_level_0__ columns because we do not need it for training\ntrain_dataset=train_dataset.remove_columns([\"__index_level_0__\"])","metadata":{"execution":{"iopub.status.busy":"2022-07-30T09:58:58.931168Z","iopub.execute_input":"2022-07-30T09:58:58.931667Z","iopub.status.idle":"2022-07-30T09:58:58.946999Z","shell.execute_reply.started":"2022-07-30T09:58:58.931621Z","shell.execute_reply":"2022-07-30T09:58:58.945525Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Import AutoTokenizer with checkpoint\"distilbert-base-uncased\"\nmy_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2022-07-30T09:58:58.948897Z","iopub.execute_input":"2022-07-30T09:58:58.949588Z","iopub.status.idle":"2022-07-30T09:59:03.034119Z","shell.execute_reply.started":"2022-07-30T09:58:58.949545Z","shell.execute_reply":"2022-07-30T09:59:03.033068Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Define the tokenization function\ndef preprocess_function(Input_Dataset):\n    return my_tokenizer(Input_Dataset[\"sentence\"], truncation=True, padding=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-30T09:59:03.035458Z","iopub.execute_input":"2022-07-30T09:59:03.035966Z","iopub.status.idle":"2022-07-30T09:59:03.040122Z","shell.execute_reply.started":"2022-07-30T09:59:03.03593Z","shell.execute_reply":"2022-07-30T09:59:03.039091Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# and use this function to tokenize the traning dataset (batch by batch if it is too large)\ntokenize_train=train_dataset.map(preprocess_function,batched=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-30T09:59:03.041573Z","iopub.execute_input":"2022-07-30T09:59:03.042062Z","iopub.status.idle":"2022-07-30T09:59:03.416692Z","shell.execute_reply.started":"2022-07-30T09:59:03.04203Z","shell.execute_reply":"2022-07-30T09:59:03.415706Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec15978ccd96401c861a0764688efa79"}},"metadata":{}}]},{"cell_type":"code","source":"# Choose the default data_collator to adapt our data to the model training\nmy_data_collator = DataCollatorWithPadding(tokenizer=my_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-07-30T09:59:03.418322Z","iopub.execute_input":"2022-07-30T09:59:03.418998Z","iopub.status.idle":"2022-07-30T09:59:03.42371Z","shell.execute_reply.started":"2022-07-30T09:59:03.41896Z","shell.execute_reply":"2022-07-30T09:59:03.42282Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Get the pre-trained model from the web (256 Mb !!!)\nmy_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4)","metadata":{"execution":{"iopub.status.busy":"2022-07-30T09:59:03.425389Z","iopub.execute_input":"2022-07-30T09:59:03.426087Z","iopub.status.idle":"2022-07-30T09:59:05.793032Z","shell.execute_reply.started":"2022-07-30T09:59:03.426048Z","shell.execute_reply":"2022-07-30T09:59:05.792077Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set the training parameters\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=7,\n    weight_decay=0.01,\n    #evaluation_strategy=\"epoch\"\n)\n\nmy_trainer = Trainer(\n    model=my_model,\n    args=training_args,\n    train_dataset=tokenize_train,\n    #eval_dataset=tokenize_test,  Here, we work with the entire dataset as training data\n    #compute_metrics=compute_metrics,\n    tokenizer=my_tokenizer,\n    data_collator=my_data_collator,\n)\n\n\nmy_trainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-07-30T09:59:05.795316Z","iopub.execute_input":"2022-07-30T09:59:05.796582Z","iopub.status.idle":"2022-07-30T10:01:30.57851Z","shell.execute_reply.started":"2022-07-30T09:59:05.796518Z","shell.execute_reply":"2022-07-30T10:01:30.577067Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 154\n  Num Epochs = 7\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 140\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [140/140 02:23, Epoch 7/7]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=140, training_loss=0.5146885463169643, metrics={'train_runtime': 144.7218, 'train_samples_per_second': 7.449, 'train_steps_per_second': 0.967, 'total_flos': 9204225229584.0, 'train_loss': 0.5146885463169643, 'epoch': 7.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Second part : define the inference function (function making the prediction from a sentence input (string)) based on the model we fine tuned above","metadata":{}},{"cell_type":"code","source":"# function made to deal with one sentence at a time. This could be re-engineered to deal with a \n# pandas series of sentences, or a list of sentences\ndef SentenceClassifier(InputSentence):\n    \"\"\" Take a sentence as input, return the corresponding label\n    \n    dependencies : my_tokenizer, my_trainer(fine tuned pre-trained model), preprocess_function\n    \"\"\"\n      \n    # here, we are keeping the input as a Dataset, which could allow us to reuse the code\n    # to answer many questions at once\n    InputSentenceDFData = {'sentence' : [InputSentence]}\n    InputSentenceDataFrame = pd.DataFrame(data = InputSentenceDFData)\n    InputSentenceDataset = datasets.Dataset.from_pandas(InputSentenceDataFrame)\n    Tokenised_InputSentence = InputSentenceDataset.map(preprocess_function, batched=False)\n    \n    LabelScores = my_trainer.predict(Tokenised_InputSentence)\n    BestLabel = LabelScores.predictions.argmax(1)\n    \n    IndexToLabel = {0:\"Software Recommendation\",1:\"Make Update\",2:\"Shutdown Computer\",3:\"Setup Printer\"}\n    OutputLabelName = IndexToLabel[BestLabel[0]]\n    \n    return OutputLabelName","metadata":{"execution":{"iopub.status.busy":"2022-07-30T10:01:30.581728Z","iopub.execute_input":"2022-07-30T10:01:30.582531Z","iopub.status.idle":"2022-07-30T10:01:30.592661Z","shell.execute_reply.started":"2022-07-30T10:01:30.58248Z","shell.execute_reply":"2022-07-30T10:01:30.591287Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"InputSentence = \"What should I use to cut pictures ?\"\nOutputLabel = SentenceClassifier(InputSentence)\nprint(f'Your question was : \"{InputSentence}\" it was classified as : \"{OutputLabel}\"')","metadata":{"execution":{"iopub.status.busy":"2022-07-30T10:01:30.594791Z","iopub.execute_input":"2022-07-30T10:01:30.595647Z","iopub.status.idle":"2022-07-30T10:01:30.749404Z","shell.execute_reply.started":"2022-07-30T10:01:30.595598Z","shell.execute_reply":"2022-07-30T10:01:30.748028Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db5137369a4547fe960e71a3c10bfc10"}},"metadata":{}},{"name":"stderr","text":"The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Prediction *****\n  Num examples = 1\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Your question was : \"What should I use to cut pictures ?\" it was classified as : \"Software Recommendation\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Third part : learn how to save and reload a saved model to avoid pre-trained model donwloading and training operations when we have a working model","metadata":{}},{"cell_type":"markdown","source":"## Save the model and tokenizer locally","metadata":{}},{"cell_type":"code","source":"ModelPath = \"/kaggle/working/model/\"\nTokenizerPath = \"/kaggle/working/tokenizer/\"","metadata":{"execution":{"iopub.status.busy":"2022-07-30T10:01:30.75095Z","iopub.execute_input":"2022-07-30T10:01:30.751404Z","iopub.status.idle":"2022-07-30T10:01:30.756791Z","shell.execute_reply.started":"2022-07-30T10:01:30.751368Z","shell.execute_reply":"2022-07-30T10:01:30.75558Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"for path_string in [ModelPath, TokenizerPath]:\n    current_path = Path(path_string)\n    if not current_path.is_dir():\n        current_path.mkdir()\n        print(f'Creation of the directory {path_string}')\n    print(f'Folder existing ? : {current_path.is_dir()}')","metadata":{"execution":{"iopub.status.busy":"2022-07-30T10:01:30.758347Z","iopub.execute_input":"2022-07-30T10:01:30.758705Z","iopub.status.idle":"2022-07-30T10:01:30.778149Z","shell.execute_reply.started":"2022-07-30T10:01:30.758673Z","shell.execute_reply":"2022-07-30T10:01:30.776812Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Folder existing ? : True\nFolder existing ? : True\n","output_type":"stream"}]},{"cell_type":"code","source":"if Path(ModelPath).is_dir():\n    my_model.save_pretrained(ModelPath)\n    print(\"model ok\")\nif Path(TokenizerPath).is_dir():\n    my_tokenizer.save_pretrained(TokenizerPath)\n    print(\"tokenizer ok\")","metadata":{"execution":{"iopub.status.busy":"2022-07-30T10:01:30.779667Z","iopub.execute_input":"2022-07-30T10:01:30.780017Z","iopub.status.idle":"2022-07-30T10:01:31.614217Z","shell.execute_reply.started":"2022-07-30T10:01:30.779986Z","shell.execute_reply":"2022-07-30T10:01:31.612624Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"Configuration saved in /kaggle/working/model/config.json\nModel weights saved in /kaggle/working/model/pytorch_model.bin\ntokenizer config file saved in /kaggle/working/tokenizer/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/tokenizer/special_tokens_map.json\n","output_type":"stream"},{"name":"stdout","text":"model ok\ntokenizer ok\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## List the files we just saved","metadata":{"execution":{"iopub.status.busy":"2022-07-30T09:43:37.31811Z","iopub.execute_input":"2022-07-30T09:43:37.318525Z","iopub.status.idle":"2022-07-30T09:43:37.325541Z","shell.execute_reply.started":"2022-07-30T09:43:37.318492Z","shell.execute_reply":"2022-07-30T09:43:37.324354Z"}}},{"cell_type":"code","source":"# option 1 : with os\nprint(os.listdir(ModelPath))\nprint(os.listdir(TokenizerPath))","metadata":{"execution":{"iopub.status.busy":"2022-07-30T10:01:31.61695Z","iopub.execute_input":"2022-07-30T10:01:31.618028Z","iopub.status.idle":"2022-07-30T10:01:31.624982Z","shell.execute_reply.started":"2022-07-30T10:01:31.61794Z","shell.execute_reply":"2022-07-30T10:01:31.623581Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"['pytorch_model.bin', 'config.json']\n['tokenizer.json', 'tokenizer_config.json', 'special_tokens_map.json', 'vocab.txt']\n","output_type":"stream"}]},{"cell_type":"code","source":"# option 2 : with pathlib\nprint([file.name for file in Path(ModelPath).iterdir()])\nprint([file.name for file in Path(TokenizerPath).iterdir()])","metadata":{"execution":{"iopub.status.busy":"2022-07-30T10:01:31.627283Z","iopub.execute_input":"2022-07-30T10:01:31.627963Z","iopub.status.idle":"2022-07-30T10:01:31.642606Z","shell.execute_reply.started":"2022-07-30T10:01:31.627871Z","shell.execute_reply":"2022-07-30T10:01:31.641287Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"['pytorch_model.bin', 'config.json']\n['tokenizer.json', 'tokenizer_config.json', 'special_tokens_map.json', 'vocab.txt']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load the local version of the model and tokenizer","metadata":{}},{"cell_type":"code","source":"LocalModel = AutoModelForSequenceClassification.from_pretrained(ModelPath, num_labels=4)\nLocalTokenizer = AutoTokenizer.from_pretrained(TokenizerPath)","metadata":{"execution":{"iopub.status.busy":"2022-07-30T10:01:31.643882Z","iopub.execute_input":"2022-07-30T10:01:31.645041Z","iopub.status.idle":"2022-07-30T10:01:32.639884Z","shell.execute_reply.started":"2022-07-30T10:01:31.644991Z","shell.execute_reply":"2022-07-30T10:01:32.638629Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"loading configuration file /kaggle/working/model/config.json\nModel config DistilBertConfig {\n  \"_name_or_path\": \"/kaggle/working/model/\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForSequenceClassification\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"problem_type\": \"single_label_classification\",\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.20.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file /kaggle/working/model/pytorch_model.bin\nAll model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n\nAll the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at /kaggle/working/model/.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\nDidn't find file /kaggle/working/tokenizer/added_tokens.json. We won't load it.\nloading file /kaggle/working/tokenizer/vocab.txt\nloading file /kaggle/working/tokenizer/tokenizer.json\nloading file None\nloading file /kaggle/working/tokenizer/special_tokens_map.json\nloading file /kaggle/working/tokenizer/tokenizer_config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"# check if the classifier works well with the local data\ndef LocalSentenceClassifier(InputSentence):\n    \"\"\" Take a sentence as input, return the corresponding label\n    \n    dependencies : LocalTokenizer, LocalModel\n    We use tokenizer2 and trainer2 instead of tokeninzer and trainer\n    to be sure that this function works with the data saved and load locally\n    \"\"\"\n    \n    Local_Trainer = Trainer(\n        model=LocalModel,\n        args=training_args,\n        train_dataset=tokenize_train,\n        #eval_dataset=tokenize_test,  Here, we work with the entire dataset as training data\n        #compute_metrics=compute_metrics,\n        tokenizer=my_tokenizer,\n        data_collator=my_data_collator,\n    )\n    \n    # here, we are keeping the input as a Dataset, which could allow us to reuse the code\n    # to answer many questions at once\n    InputSentenceDFData = {'sentence' : [InputSentence]}\n    InputSentenceDataFrame = pd.DataFrame(data = InputSentenceDFData)\n    InputSentenceDataset = datasets.Dataset.from_pandas(InputSentenceDataFrame)\n    Tokenised_InputSentence = InputSentenceDataset.map(preprocess_function,batched=False)\n    \n    LabelScores = Local_Trainer.predict(Tokenised_InputSentence)\n    BestLabel = LabelScores.predictions.argmax(1)\n    \n    IndexToLabel = {0:\"Software Recommendation\",1:\"Make Update\",2:\"Shutdown Computer\",3:\"Setup Printer\"}\n    OutputLabelName = IndexToLabel[BestLabel[0]]\n    \n    return OutputLabelName","metadata":{"execution":{"iopub.status.busy":"2022-07-30T10:19:03.309242Z","iopub.execute_input":"2022-07-30T10:19:03.309978Z","iopub.status.idle":"2022-07-30T10:19:03.322261Z","shell.execute_reply.started":"2022-07-30T10:19:03.309924Z","shell.execute_reply":"2022-07-30T10:19:03.321139Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"InputSentence = \"How can I update Ubuntu ?\"\nOutputLabel = LocalSentenceClassifier(InputSentence)\nprint(f'Your question was : \"{InputSentence}\" it was classified as : \"{OutputLabel}\"')","metadata":{"execution":{"iopub.status.busy":"2022-07-30T10:19:05.643377Z","iopub.execute_input":"2022-07-30T10:19:05.644221Z","iopub.status.idle":"2022-07-30T10:19:05.812324Z","shell.execute_reply.started":"2022-07-30T10:19:05.644157Z","shell.execute_reply":"2022-07-30T10:19:05.810779Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61a7048204a945428339298ed4c00088"}},"metadata":{}},{"name":"stderr","text":"The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Prediction *****\n  Num examples = 1\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Your question was : \"How can I update Ubuntu ?\" it was classified as : \"Software Recommendation\"\n","output_type":"stream"}]}]}