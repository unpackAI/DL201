{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b990127e",
   "metadata": {},
   "source": [
    "## üíª UnpackAI DL201 Bootcamp - Week 1 - Skills: NLP\n",
    "\n",
    "### üìï Learning Objectives\n",
    "\n",
    "* Solidify the basic notion of NLP and how it can be applied to a variety of tasks.\n",
    "* Practice using Pandas for loading and processing text data.\n",
    "* Ilustrate the process of converting a text document into a dataframe and from there into a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2717a57",
   "metadata": {},
   "source": [
    "### A basic NLP Overview\n",
    "\n",
    "From Wikipedia:\n",
    "- \"Natural language processing (NLP) is a subfield of **linguistics, computer science, and artificial intelligence** concerned with the interactions between computers and **human language**, in particular how to program computers to **process and analyze** large amounts of natural language data. The goal is a computer capable of **\"understanding\"** the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\"\n",
    "\n",
    "- Approaches to NLP tasks:\n",
    "    - Rule-based\n",
    "    - Traditional machine learning\n",
    "    - Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a04c7e",
   "metadata": {},
   "source": [
    "In NLP, we often need to perform text preprocessing, such as removing stop words, stemming, lemmatization, and tokenization.\n",
    "A nice overview is presented in: \n",
    "- https://stanfordnlp.github.io/CoreNLP/ \n",
    "- https://www.techtarget.com/searchenterpriseai/definition/natural-language-processing-NLP\n",
    "\n",
    "Common NLP tasks:\n",
    "- Classification\n",
    "- Masked filing\n",
    "- Text prediction\n",
    "- Sentiment analysis\n",
    "    - Positive\n",
    "    - Negative\n",
    "    - Subjectivity\n",
    "- Entity recognition\n",
    "    - Person\n",
    "    - Location\n",
    "    - Organization\n",
    "- Entity extraction\n",
    "- Keyword extraction\n",
    "- Topic extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f71e40e",
   "metadata": {},
   "source": [
    "### Ilustrative example\n",
    "\n",
    "Below there is a code example that that illustrates the usage of Pandas for text manipulation and a few exploratory steps to create Tensors representing the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b9dbbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "! pip install transformers openpyxl\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe83bb",
   "metadata": {},
   "source": [
    "It is important to set correctly your data folder path as a local variable, depending on where you run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91be4bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data directory is d:\\GitHub\\DL201\\data\\nlp\n"
     ]
    }
   ],
   "source": [
    "# Set the data directory path as a variable\n",
    "\n",
    "# Uncomment this for Kaggle\n",
    "!git clone https://github.com/unpackAI/DL201.git\n",
    "from pathlib import Path\n",
    "DATA_DIR = Path('/kaggle/working/DL201/data/nlp') #uncomment for kaggle\n",
    "\n",
    "# Uncomment this for local\n",
    "# os.chdir('../data/nlp')\n",
    "# DATA_DIR = os.getcwd()\n",
    "\n",
    "print(f'data directory is {DATA_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfe8f1d",
   "metadata": {},
   "source": [
    "Let's load a sample text file and feed it into the BERT model. The data/nlp folder of the repository contains a txt file with sentences from a book, the book was taken from: http://www.textfiles.com/stories/. Feel free to download a different book and use it when explolring this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5340ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample text, from the data folder\n",
    "os.chdir(DATA_DIR)\n",
    "sample_text = open('alad10.txt').read()\n",
    "\n",
    "# Split the text into sentences\n",
    "sentences = sample_text.split('\\n')\n",
    "\n",
    "# Load the sentences into a dataframe\n",
    "df = pd.DataFrame(sentences, columns=['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8f56a9",
   "metadata": {},
   "source": [
    "As it has been reitared before, loading the data into Pandas gives us tremendous flexibility to perform data cleaning and preprocessing with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a8c3329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>merchandise.  Next day he bought Aladdin a fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>out the oil it contains, and bring it me.\"  He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>still wore.  The genie he had seen in the cave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Immediately an enormous and frightful genie ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>be surprised at not having seen him before, as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>lamp and kill him afterwards.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>what had become of his palace, but they only l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>and lastly, ten thousand pieces of gold in ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>offering to exchange fine new lamps for old on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>Aladdin went back to the Princess, saying his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>took him all over the city, showing him the si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\"but he died a long while ago.\"  On this the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>been deceived.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>capital of China, bent on Aladdin's ruin.  As ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>out the foolish Aladdin for this purpose, inte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence\n",
       "25   merchandise.  Next day he bought Aladdin a fin...\n",
       "56   out the oil it contains, and bring it me.\"  He...\n",
       "346  still wore.  The genie he had seen in the cave...\n",
       "78   Immediately an enormous and frightful genie ro...\n",
       "20   be surprised at not having seen him before, as...\n",
       "73                       lamp and kill him afterwards.\n",
       "343  what had become of his palace, but they only l...\n",
       "222  and lastly, ten thousand pieces of gold in ten...\n",
       "302  offering to exchange fine new lamps for old on...\n",
       "464  Aladdin went back to the Princess, saying his ...\n",
       "26   took him all over the city, showing him the si...\n",
       "11   \"but he died a long while ago.\"  On this the s...\n",
       "471                                     been deceived.\n",
       "285  capital of China, bent on Aladdin's ruin.  As ...\n",
       "72   out the foolish Aladdin for this purpose, inte..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect some of the sentences\n",
    "df.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19f93c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the sentences that have less than 3 words\n",
    "df = df[df['sentence'].str.split().str.len() > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8f87549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenis\\AppData\\Local\\Temp/ipykernel_18116/4114492291.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['sentence'] = df['sentence'].str.replace('[^\\w\\s]','')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>sighed deeply and at last told her mother how ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>ordered the executioner to cut off his head  T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>son and the Princess  Take this newmarried man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>There is only one thing that surprises me  Was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>Next day Aladdin invited the Sultan to see the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>between them  Then they journeyed onwards till...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>that he came to no harm  He was carried before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>I would do a great deal more than that for the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>he went home but fainted on the threshold  Whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>but a wicked magician and told her of how she had</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence\n",
       "178  sighed deeply and at last told her mother how ...\n",
       "324  ordered the executioner to cut off his head  T...\n",
       "162  son and the Princess  Take this newmarried man...\n",
       "259  There is only one thing that surprises me  Was...\n",
       "256  Next day Aladdin invited the Sultan to see the...\n",
       "32   between them  Then they journeyed onwards till...\n",
       "323  that he came to no harm  He was carried before...\n",
       "207  I would do a great deal more than that for the...\n",
       "83   he went home but fainted on the threshold  Whe...\n",
       "470  but a wicked magician and told her of how she had"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation from all sentences\n",
    "df['sentence'] = df['sentence'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Note: instead of regex a list of punctuation can be used, give it a try!\n",
    "punctuation = [\n",
    "    '.', ',', '!', '?', ':', ';', '\"', \"'\", '-', '_', '(', ')', '[', ']', '{', '}', '#', '@', '$', '%', '^', '&', '*',\n",
    "     '+', '=', '<', '>', '/', '\\\\', '|', '~', '`', '‚Äú', '‚Äù', '‚Äò', '‚Äô'\n",
    "]\n",
    "\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aedee9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>daughter happened too look up and rubbed his e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>not but he will use violence  aladdin comforte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>the sultan sent musicians with trumpets and cy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>hanging from the dome  if that is all replied ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>genie had brought aladdin sold one of the silv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>next morning the sultan looked out of the wind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>bowl twelve silver plates containing rich meat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>and led him into a hall where a feast was spre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>him what he thought of it  it is truly beautif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>notice of her  she went every day for a week a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence\n",
       "411  daughter happened too look up and rubbed his e...\n",
       "379  not but he will use violence  aladdin comforte...\n",
       "243  the sultan sent musicians with trumpets and cy...\n",
       "449  hanging from the dome  if that is all replied ...\n",
       "102  genie had brought aladdin sold one of the silv...\n",
       "315  next morning the sultan looked out of the wind...\n",
       "94   bowl twelve silver plates containing rich meat...\n",
       "227  and led him into a hall where a feast was spre...\n",
       "439  him what he thought of it  it is truly beautif...\n",
       "124  notice of her  she went every day for a week a..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert all sentences to lowercase\n",
    "df['sentence'] = df['sentence'].str.lower()\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7bc800",
   "metadata": {},
   "source": [
    "**Sentences are a key unit of information when it comes to NLP** (as wells as tokens) in order to represent our data as a uniform \"block\" of text, we need to find out our longest sentence, the rest of them will later be padded with padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c3fdce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length is 75\n"
     ]
    }
   ],
   "source": [
    "# Get the length of the longest senctence\n",
    "max_len = df['sentence'].str.len().max()\n",
    "print(f'max sentence length is {max_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e70bd",
   "metadata": {},
   "source": [
    "The transformers library provides a convenient way to load a variety of BERT models. Let's first load and explore a tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6b2c34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fa75445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size is: 30522\n"
     ]
    }
   ],
   "source": [
    "# Get the tokenizer vocabulary words\n",
    "vocab = bert_tokenizer.vocab\n",
    "vocab_size = len(vocab)\n",
    "print(f'vocab size is: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e209cb71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28797</th>\n",
       "      <td>subtly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8080</th>\n",
       "      <td>monitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16606</th>\n",
       "      <td>dukes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>can</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3346</th>\n",
       "      <td>stadium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16281</th>\n",
       "      <td>##iling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15484</th>\n",
       "      <td>dispersed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22312</th>\n",
       "      <td>blitz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>Èáë</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24292</th>\n",
       "      <td>##rith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28566</th>\n",
       "      <td>maui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10053</th>\n",
       "      <td>bolt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29824</th>\n",
       "      <td>##ÿ≥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9784</th>\n",
       "      <td>slowed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25686</th>\n",
       "      <td>scully</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tokens\n",
       "28797     subtly\n",
       "8080     monitor\n",
       "16606      dukes\n",
       "2064         can\n",
       "3346     stadium\n",
       "16281    ##iling\n",
       "15484  dispersed\n",
       "22312      blitz\n",
       "1964           Èáë\n",
       "24292     ##rith\n",
       "28566       maui\n",
       "10053       bolt\n",
       "29824        ##ÿ≥\n",
       "9784      slowed\n",
       "25686     scully"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the vocabulary words as a list, load them into a dataframe\n",
    "vocab_list = list(vocab.keys())\n",
    "vocab_df = pd.DataFrame(vocab_list, columns=['tokens'])\n",
    "vocab_df.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9146db8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 995 tokens that begin with \"unused\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>[unused383]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>[unused89]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>[unused264]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>[unused332]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>[unused453]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>[unused868]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>[unused250]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>[unused159]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>[unused115]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>[unused512]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tokens\n",
       "388  [unused383]\n",
       "90    [unused89]\n",
       "269  [unused264]\n",
       "337  [unused332]\n",
       "458  [unused453]\n",
       "873  [unused868]\n",
       "255  [unused250]\n",
       "164  [unused159]\n",
       "120  [unused115]\n",
       "517  [unused512]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the count of tokens that begin with 'UNUSED'\n",
    "unused_tokens = vocab_df[vocab_df['tokens'].str.find('unused')>=0]\n",
    "print(f'There are {len(unused_tokens)} tokens that begin with \"unused\"')\n",
    "unused_tokens.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8e9effe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 997 tokens that have a size of 1 character\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1832</th>\n",
       "      <td>Â≤°</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>’´</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>‡∑è</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>‡ºç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>”è</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1925</th>\n",
       "      <td>Á•û</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>\\</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>‰ªÆ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tokens\n",
       "1048      l\n",
       "1832      Â≤°\n",
       "1225      ’´\n",
       "1408      ‡∑è\n",
       "1425      ‡ºç\n",
       "1019      5\n",
       "1218      ”è\n",
       "1925      Á•û\n",
       "1032      \\\n",
       "1761      ‰ªÆ"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the tokens that have a size of 1 character\n",
    "one_char_tokens = vocab_df[vocab_df['tokens'].str.len()==1]\n",
    "print(f'There are {len(one_char_tokens)} tokens that have a size of 1 character')\n",
    "one_char_tokens.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ae4e532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 28042 tokens that likely reprensent English words\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8790</th>\n",
       "      <td>dynamic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18415</th>\n",
       "      <td>truce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4269</th>\n",
       "      <td>begins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12197</th>\n",
       "      <td>watches</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25778</th>\n",
       "      <td>##col</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5984</th>\n",
       "      <td>puerto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22495</th>\n",
       "      <td>distributors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8872</th>\n",
       "      <td>cop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25743</th>\n",
       "      <td>helix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8025</th>\n",
       "      <td>curious</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tokens\n",
       "8790        dynamic\n",
       "18415         truce\n",
       "4269         begins\n",
       "12197       watches\n",
       "25778         ##col\n",
       "5984         puerto\n",
       "22495  distributors\n",
       "8872            cop\n",
       "25743         helix\n",
       "8025        curious"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the tokens which have a size of more than 2 characters and does not contain the word 'unused'\n",
    "two_char_tokens = vocab_df[(vocab_df['tokens'].str.len()>2) & (vocab_df['tokens'].str.find('unused')<0)]\n",
    "print(f'There are {len(two_char_tokens)} tokens that likely reprensent English words')\n",
    "two_char_tokens.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dc784c",
   "metadata": {},
   "source": [
    "Each sentence is currently represented as a list of characters. We need to transform this into a list of tokens, tokens then get converted into numbers using the tokenizers vocabulary as indexes. Here is an example with a phrase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "826a1502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample sentence is:\n",
      "This is a sample sentence, which we will tokenize using the BERT tokenizer.\n",
      "\n",
      "The tokenized sentence is:\n",
      "['this', 'is', 'a', 'sample', 'sentence', ',', 'which', 'we', 'will', 'token', '##ize', 'using', 'the', 'bert', 'token', '##izer', '.']\n",
      "\n",
      "The numericalized sentence is:\n",
      "[2023, 2003, 1037, 7099, 6251, 1010, 2029, 2057, 2097, 19204, 4697, 2478, 1996, 14324, 19204, 17629, 1012]\n"
     ]
    }
   ],
   "source": [
    "# Example of tokenizing a sentence\n",
    "sample_sentence = \"This is a sample sentence, which we will tokenize using the BERT tokenizer.\"\n",
    "print(f'The sample sentence is:\\n{sample_sentence}')\n",
    "\n",
    "tokenized_sentence = bert_tokenizer.tokenize(sample_sentence)\n",
    "print(f'\\nThe tokenized sentence is:\\n{tokenized_sentence}')\n",
    "\n",
    "numericalized_sentence = bert_tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "print(f'\\nThe numericalized sentence is:\\n{numericalized_sentence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79392b66",
   "metadata": {},
   "source": [
    "We should now do the same for the sentences in the dataframe. Before proceding is a good idea to create a copy of what we have so far to be able to revert back to the original dataframe in case we need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "205585c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the senctences dataframe\n",
    "tokens_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47e1b1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tokenized_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>that i may find out what she wants  next day a...</td>\n",
       "      <td>[that, i, may, find, out, what, she, wants, ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>prosperity  when he had done the princess made...</td>\n",
       "      <td>[prosperity, when, he, had, done, the, princes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>with her at first sight  he went home so chang...</td>\n",
       "      <td>[with, her, at, first, sight, he, went, home, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>stood the palace as before  he hastened thithe...</td>\n",
       "      <td>[stood, the, palace, as, before, he, haste, ##...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>asked his will  save my life genie said aladdi...</td>\n",
       "      <td>[asked, his, will, save, my, life, genie, said...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>hand of the princess  now i pray you to forgiv...</td>\n",
       "      <td>[hand, of, the, princess, now, i, pray, you, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>every day carrying something in a napkin  call...</td>\n",
       "      <td>[every, day, carrying, something, in, a, napki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>the people however who loved him followed arme...</td>\n",
       "      <td>[the, people, however, who, loved, him, follow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>before and the sultan who had forgotten aladdi...</td>\n",
       "      <td>[before, and, the, sultan, who, had, forgotten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>another set of plates and thus they lived many...</td>\n",
       "      <td>[another, set, of, plates, and, thus, they, li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  \\\n",
       "128  that i may find out what she wants  next day a...   \n",
       "435  prosperity  when he had done the princess made...   \n",
       "113  with her at first sight  he went home so chang...   \n",
       "412  stood the palace as before  he hastened thithe...   \n",
       "347  asked his will  save my life genie said aladdi...   \n",
       "137  hand of the princess  now i pray you to forgiv...   \n",
       "127  every day carrying something in a napkin  call...   \n",
       "322  the people however who loved him followed arme...   \n",
       "194  before and the sultan who had forgotten aladdi...   \n",
       "104  another set of plates and thus they lived many...   \n",
       "\n",
       "                                    tokenized_sentence  \n",
       "128  [that, i, may, find, out, what, she, wants, ne...  \n",
       "435  [prosperity, when, he, had, done, the, princes...  \n",
       "113  [with, her, at, first, sight, he, went, home, ...  \n",
       "412  [stood, the, palace, as, before, he, haste, ##...  \n",
       "347  [asked, his, will, save, my, life, genie, said...  \n",
       "137  [hand, of, the, princess, now, i, pray, you, t...  \n",
       "127  [every, day, carrying, something, in, a, napki...  \n",
       "322  [the, people, however, who, loved, him, follow...  \n",
       "194  [before, and, the, sultan, who, had, forgotten...  \n",
       "104  [another, set, of, plates, and, thus, they, li...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize each sentence in the dataframe\n",
    "tokens_df['tokenized_sentence'] = tokens_df['sentence'].apply(bert_tokenizer.tokenize)\n",
    "tokens_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a113239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tokenized_sentence</th>\n",
       "      <th>numericalized_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>its value laughingly bade the slave take it an...</td>\n",
       "      <td>[its, value, laughing, ##ly, bad, ##e, the, sl...</td>\n",
       "      <td>[2049, 3643, 5870, 2135, 2919, 2063, 1996, 665...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>must remember his promises and i will remember...</td>\n",
       "      <td>[must, remember, his, promises, and, i, will, ...</td>\n",
       "      <td>[2442, 3342, 2010, 10659, 1998, 1045, 2097, 33...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>after this aladdin and his wife lived in peace</td>\n",
       "      <td>[after, this, ala, ##ddin, and, his, wife, liv...</td>\n",
       "      <td>[2044, 2023, 21862, 18277, 1998, 2010, 2564, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>rubies diamonds and emeralds he cried it is a ...</td>\n",
       "      <td>[rub, ##ies, diamonds, and, emerald, ##s, he, ...</td>\n",
       "      <td>[14548, 3111, 11719, 1998, 14110, 2015, 2002, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>told saying the names of his father and grandf...</td>\n",
       "      <td>[told, saying, the, names, of, his, father, an...</td>\n",
       "      <td>[2409, 3038, 1996, 3415, 1997, 2010, 2269, 199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>deserve to be burnt to ashes but that this req...</td>\n",
       "      <td>[deserve, to, be, burnt, to, ashes, but, that,...</td>\n",
       "      <td>[10107, 2000, 2022, 11060, 2000, 11289, 2021, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>while the magician drained his to the dregs an...</td>\n",
       "      <td>[while, the, magician, drained, his, to, the, ...</td>\n",
       "      <td>[2096, 1996, 16669, 11055, 2010, 2000, 1996, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>was frightened  he told her he loved the princ...</td>\n",
       "      <td>[was, frightened, he, told, her, he, loved, th...</td>\n",
       "      <td>[2001, 10363, 2002, 2409, 2014, 2002, 3866, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>and told his mother of his newly found uncle  ...</td>\n",
       "      <td>[and, told, his, mother, of, his, newly, found...</td>\n",
       "      <td>[1998, 2409, 2010, 2388, 1997, 2010, 4397, 217...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>the mountains  aladdin was so tired that he be...</td>\n",
       "      <td>[the, mountains, ala, ##ddin, was, so, tired, ...</td>\n",
       "      <td>[1996, 4020, 21862, 18277, 2001, 2061, 5458, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  \\\n",
       "306  its value laughingly bade the slave take it an...   \n",
       "200  must remember his promises and i will remember...   \n",
       "473     after this aladdin and his wife lived in peace   \n",
       "258  rubies diamonds and emeralds he cried it is a ...   \n",
       "49   told saying the names of his father and grandf...   \n",
       "457  deserve to be burnt to ashes but that this req...   \n",
       "403  while the magician drained his to the dregs an...   \n",
       "114  was frightened  he told her he loved the princ...   \n",
       "15   and told his mother of his newly found uncle  ...   \n",
       "33   the mountains  aladdin was so tired that he be...   \n",
       "\n",
       "                                    tokenized_sentence  \\\n",
       "306  [its, value, laughing, ##ly, bad, ##e, the, sl...   \n",
       "200  [must, remember, his, promises, and, i, will, ...   \n",
       "473  [after, this, ala, ##ddin, and, his, wife, liv...   \n",
       "258  [rub, ##ies, diamonds, and, emerald, ##s, he, ...   \n",
       "49   [told, saying, the, names, of, his, father, an...   \n",
       "457  [deserve, to, be, burnt, to, ashes, but, that,...   \n",
       "403  [while, the, magician, drained, his, to, the, ...   \n",
       "114  [was, frightened, he, told, her, he, loved, th...   \n",
       "15   [and, told, his, mother, of, his, newly, found...   \n",
       "33   [the, mountains, ala, ##ddin, was, so, tired, ...   \n",
       "\n",
       "                                numericalized_sentence  \n",
       "306  [2049, 3643, 5870, 2135, 2919, 2063, 1996, 665...  \n",
       "200  [2442, 3342, 2010, 10659, 1998, 1045, 2097, 33...  \n",
       "473  [2044, 2023, 21862, 18277, 1998, 2010, 2564, 2...  \n",
       "258  [14548, 3111, 11719, 1998, 14110, 2015, 2002, ...  \n",
       "49   [2409, 3038, 1996, 3415, 1997, 2010, 2269, 199...  \n",
       "457  [10107, 2000, 2022, 11060, 2000, 11289, 2021, ...  \n",
       "403  [2096, 1996, 16669, 11055, 2010, 2000, 1996, 2...  \n",
       "114  [2001, 10363, 2002, 2409, 2014, 2002, 3866, 19...  \n",
       "15   [1998, 2409, 2010, 2388, 1997, 2010, 4397, 217...  \n",
       "33   [1996, 4020, 21862, 18277, 2001, 2061, 5458, 2...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the numericalized sentences to the dataframe\n",
    "tokens_df['numericalized_sentence'] = tokens_df['tokenized_sentence'].apply(bert_tokenizer.convert_tokens_to_ids)\n",
    "tokens_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8de28c0",
   "metadata": {},
   "source": [
    "Phrases that will be inputted to a BERT model must include the special tokens `[CLS]` and `[SEP]`. These tokens are used to indicate the start and end of the input sequence. Let's add these tokens to the sample phrase. Another special token is `[PAD]`, which is used to pad shorter sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c621a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The tokenized sentence is:\n",
      "['CLS', 'this', 'is', 'a', 'sample', 'sentence', ',', 'which', 'we', 'will', 'token', '##ize', 'using', 'the', 'bert', 'token', '##izer', '.', 'SEP']\n",
      "\n",
      "The numericalized sentence is:\n",
      "[100, 2023, 2003, 1037, 7099, 6251, 1010, 2029, 2057, 2097, 19204, 4697, 2478, 1996, 14324, 19204, 17629, 1012, 100]\n",
      "- The token ID for the special token [CLS] is: 101\n",
      "- The token ID for the special token [SEP] is: 102\n",
      "- The token ID for the special token [PAD] is: 0\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence = ['CLS'] + tokenized_sentence + ['SEP']\n",
    "print(f'\\nThe tokenized sentence is:\\n{tokenized_sentence}')\n",
    "\n",
    "numericalized_sentence = bert_tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "print(f'\\nThe numericalized sentence is:\\n{numericalized_sentence}')\n",
    "\n",
    "# Print the IDs for the special tokens for the BERT model\n",
    "print(f'- The token ID for the special token [CLS] is: {bert_tokenizer.cls_token_id}')\n",
    "print(f'- The token ID for the special token [SEP] is: {bert_tokenizer.sep_token_id}')\n",
    "print(f'- The token ID for the special token [PAD] is: {bert_tokenizer.pad_token_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176fd665",
   "metadata": {},
   "source": [
    "As the exampled indicates, we need to add the [CLS] and [SEP] tokens and tokenize each sentence of the text dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82dd7f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327    [101, 3140, 2037, 2126, 2046, 1996, 10119, 199...\n",
       "229    [101, 3038, 1045, 2442, 3857, 1037, 4186, 4906...\n",
       "38     [101, 8587, 2039, 12668, 2096, 1045, 2785, 257...\n",
       "301    [101, 21658, 3880, 1996, 6658, 2040, 2064, 239...\n",
       "127    [101, 2296, 2154, 4755, 2242, 1999, 1037, 2061...\n",
       "386    [101, 2010, 2406, 2002, 2097, 2175, 2005, 2070...\n",
       "195    [101, 4622, 2032, 1998, 2741, 2005, 2014, 2006...\n",
       "438    [101, 3571, 1997, 5456, 1996, 4615, 3662, 2032...\n",
       "76     [101, 2012, 2197, 2002, 16763, 2010, 2398, 199...\n",
       "103    [101, 2127, 3904, 2020, 2187, 2002, 2059, 2018...\n",
       "Name: numericalized_sentence, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the 100 special tokens to the numericalized sentences on the dataframe\n",
    "tokens_df['numericalized_sentence'] = tokens_df['numericalized_sentence'].apply(lambda x: [bert_tokenizer.cls_token_id] + x + [bert_tokenizer.sep_token_id])\n",
    "tokens_df['numericalized_sentence'].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62b1d86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the 0 padding to the numericalized sentences on the dataframe\n",
    "tokens_df['numericalized_sentence'] = tokens_df['numericalized_sentence'].apply(lambda x: x + [bert_tokenizer.pad_token_id] * (max_len - len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1360e61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85     75\n",
       "80     75\n",
       "311    75\n",
       "181    75\n",
       "290    75\n",
       "228    75\n",
       "242    75\n",
       "34     75\n",
       "279    75\n",
       "263    75\n",
       "Name: numericalized_sentence_length, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a new column that indicates the length of the numericalized sentences\n",
    "tokens_df['numericalized_sentence_length'] = tokens_df['numericalized_sentence'].apply(len)\n",
    "tokens_df['numericalized_sentence_length'] .sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb1ed282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(447,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the numericalized sentences from the dataframe\n",
    "numericalized_sentences = tokens_df['numericalized_sentence'].values\n",
    "numericalized_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2344b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each row of the numericalized sentences to a list\n",
    "numericalized_sentences = [list(x) for x in numericalized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "feeed5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the numericalized sentences is: (447, 75)\n",
      "[[  101 21862 18277 ...     0     0     0]\n",
      " [  101  2045  2320 ...     0     0     0]\n",
      " [  101  1037 23358 ...     0     0     0]\n",
      " ...\n",
      " [  101  2044  2023 ...     0     0     0]\n",
      " [  101  2002  4594 ...     0     0     0]\n",
      " [  101  2005  2116 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# Convert the list into a 2D NumPy array\n",
    "numericalized_sentences = np.array(numericalized_sentences)\n",
    "print(f'The shape of the numericalized sentences is: {numericalized_sentences.shape}')\n",
    "print(numericalized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "465072a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101, 21862, 18277,  ...,     0,     0,     0],\n",
      "        [  101,  2045,  2320,  ...,     0,     0,     0],\n",
      "        [  101,  1037, 23358,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2044,  2023,  ...,     0,     0,     0],\n",
      "        [  101,  2002,  4594,  ...,     0,     0,     0],\n",
      "        [  101,  2005,  2116,  ...,     0,     0,     0]], dtype=torch.int32)\n",
      "the shape of the numericalized tensor is: torch.Size([447, 75])\n"
     ]
    }
   ],
   "source": [
    "#  Convert the numpy array into a Tensor\n",
    "numericalized_sentences = torch.from_numpy(numericalized_sentences)\n",
    "print(numericalized_sentences)\n",
    "print(f'the shape of the numericalized tensor is: {numericalized_sentences.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467c9431",
   "metadata": {},
   "source": [
    "### Discuss the following:\n",
    "* What whas the pipeline of this exercise?\n",
    "* A Summary of the data cleaning and preprocessing steps.\n",
    "* What is the difference between a token and a sentence?\n",
    "* Why did we converted the tokens to numbers?\n",
    "* Why did we add the special tokens?\n",
    "* What advantages offered Pandas for text manipulation?\n",
    "* Would this approach be suitable for complex datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d52596f",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "* Repeat this pipeline with 3 different books that appear very different in nature (don't add the special tokens).\n",
    "* When you obtain the numericalized sentences, convert them into a long 1D Numpy array.\n",
    "* Plot the distribution of the numericalized tokens for each book using histograms.\n",
    "* Comment your experience during the next lesson."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3bd52410ac2df406f4a7dd1b627ebb13ba3e371ebde73b2ddba1931965b1f80d"
  },
  "kernelspec": {
   "display_name": "unpackAIdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
