{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a244d1",
   "metadata": {},
   "source": [
    "## üíª UnpackAI DL201 Bootcamp - Week 1 - Skills: NLP\n",
    "\n",
    "### üìï Learning Objectives\n",
    "\n",
    "* Gain an appreciation for how NLP models accept data\n",
    "* To see past the complexity and extract text from common file formats\n",
    "* Perform cursory EDA by using a Pandas Series while expanding confidence and awareness of it's capabilities\n",
    "\n",
    "### üìñ Concepts map\n",
    "* Structured vs. Unstructured Data\n",
    "* Qualitative vs Quantitative Data\n",
    "* Flattening Data Structures\n",
    "* Tokenization\n",
    "* Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad5880b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers openpyxl docx -qq\n",
    "!git clone https://github.com/unpackAI/DL201.git\n",
    "\n",
    "\n",
    "# Imports \n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import requests\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "#Kaggle config\n",
    "DATA_DIR = Path('/kaggle/working/DL201/data') #uncomment for kaggle\n",
    "IMAGE_DIR = Path('/kaggle/working/DL201/img') #Uncomment for Kaggle\n",
    "\n",
    "\n",
    "# Local Config\n",
    "#DATA_DIR = Path.home()/'Datasets'/'unpackAI'/'DL201'/'data'\n",
    "#IMAGE_DIR = Path('../img') #uncomment for local machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438353f",
   "metadata": {},
   "source": [
    "# Why is NLP Data Notorious?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720283fe",
   "metadata": {},
   "source": [
    "The nature of text data makes it intimidating compared to other forms of data. It is not too difficult to consider an image or dataframe because we are familiar with these from everyday life. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0b2c9f",
   "metadata": {},
   "source": [
    "## Text is Unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d633438",
   "metadata": {},
   "source": [
    "Instead, Text data can take endless forms because it represents ideas. As a result, text and words, are very free. They can be as short as an utterance, to as long as a dictionary. They can be stored as scanned PDFs of archives written with typewriters, word documents, e-books, front end web pages, back end API responses, or even just plain .txt files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62355f23",
   "metadata": {},
   "source": [
    "## Text is Qualitative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450c85f2",
   "metadata": {},
   "source": [
    "Text doesn't have the meaning of numbers, and can be interpreted in different ways. \n",
    "\n",
    "For example, a mountain is normally a noun, but it is an adjective in mountain lion, and a mountain lion is also called a puma or cougar. There is inherent ambiguity in language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8156e9d",
   "metadata": {},
   "source": [
    "# Part 1: How NLP Quantifies Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2717a57",
   "metadata": {},
   "source": [
    "### A basic NLP Overview\n",
    "\n",
    "From Wikipedia:\n",
    "- \"Natural language processing (NLP) is a subfield of **linguistics, computer science, and artificial intelligence** concerned with the interactions between computers and **human language**, in particular how to program computers to **process and analyze** large amounts of natural language data. The goal is a computer capable of **\"understanding\"** the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\"\n",
    "\n",
    "- Approaches to NLP tasks:\n",
    "    - Rule-based\n",
    "    - Traditional machine learning\n",
    "    - Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a04c7e",
   "metadata": {},
   "source": [
    "In NLP, we often need to perform text preprocessing, such as removing stop words, stemming, lemmatization, and tokenization.\n",
    "A nice overview is presented in: \n",
    "- https://stanfordnlp.github.io/CoreNLP/ \n",
    "- https://www.techtarget.com/searchenterpriseai/definition/natural-language-processing-NLP\n",
    "\n",
    "Common NLP tasks:\n",
    "- Classification\n",
    "- Masked filing\n",
    "- Text prediction\n",
    "- Sentiment analysis\n",
    "    - Positive\n",
    "    - Negative\n",
    "    - Subjectivity\n",
    "- Entity recognition\n",
    "    - Person\n",
    "    - Location\n",
    "    - Organization\n",
    "- Entity extraction\n",
    "- Keyword extraction\n",
    "- Topic extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f71e40e",
   "metadata": {},
   "source": [
    "### Ilustrative example\n",
    "\n",
    "Below there is a code example that that illustrates the usage of Pandas for text manipulation and a few exploratory steps to create Tensors representing the text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfe8f1d",
   "metadata": {},
   "source": [
    "Let's load a sample book conveniently available in txt format from the collection at http://www.textfiles.com/stories/ the book in this case is Aladdin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5340ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample text, from the provided url\n",
    "response = requests.get('http://www.textfiles.com/stories/alad10.txt')\n",
    "sample_text = response.text\n",
    "\n",
    "# Split the text into sentences\n",
    "sentences = sample_text.split('\\n')\n",
    "\n",
    "# Load the sentences into a dataframe\n",
    "df = pd.DataFrame(sentences, columns=['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8f56a9",
   "metadata": {},
   "source": [
    "As it has been reitared before, loading the data into Pandas gives us tremendous flexibility to perform data cleaning and preprocessing with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c3329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>bowl, twelve silver plates containing rich mea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>desperate deed if I refused to go and ask your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>the Princess.  \"Fear nothing,\" Aladdin said to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>the mountains.  Aladdin was so tired that he b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>passed there. Her mother did not believe her i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Aladdin did not mend his ways.  One day, when ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>people by her touch of their ailments, whereup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>way and ordered Aladdin to be unbound, and par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>lamp and kill him afterwards.\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>more wicked and more cunning than himself.  He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\"I am your uncle, and knew you from your liken...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>fruit off the trees, and, having got the lamp,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>and Aladdin told his mother about the lamp.  S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>battles for him, but remained as courteous as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>cotton, for he would sell the lamp instead.  A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence\n",
       "94   bowl, twelve silver plates containing rich mea...\n",
       "136  desperate deed if I refused to go and ask your...\n",
       "165  the Princess.  \"Fear nothing,\" Aladdin said to...\n",
       "33   the mountains.  Aladdin was so tired that he b...\n",
       "180  passed there. Her mother did not believe her i...\n",
       "8    Aladdin did not mend his ways.  One day, when ...\n",
       "432  people by her touch of their ailments, whereup...\n",
       "330  way and ordered Aladdin to be unbound, and par...\n",
       "73                     lamp and kill him afterwards.\\r\n",
       "420  more wicked and more cunning than himself.  He...\n",
       "13   \"I am your uncle, and knew you from your liken...\n",
       "60   fruit off the trees, and, having got the lamp,...\n",
       "98   and Aladdin told his mother about the lamp.  S...\n",
       "276  battles for him, but remained as courteous as ...\n",
       "89   cotton, for he would sell the lamp instead.  A..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect some of the sentences\n",
    "df.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f93c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the sentences that have less than 3 words\n",
    "df = df[df['sentence'].str.split().str.len() > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f87549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenis\\AppData\\Local\\Temp/ipykernel_10168/4114492291.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['sentence'] = df['sentence'].str.replace('[^\\w\\s]','')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Aladdin at last prevailed upon her to go befor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>carry his request  She fetched a napkin and la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>Besides this six slaves beautifully dressed to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>treasure Aladdin forgot his fears and grasped ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>amazed he could not say a word  Where is your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>and filled up the small house and garden  Alad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>spokesman we cannot find jewels enough  The Su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>deserve to be burnt to ashes but that this req...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>at nightfall to his mother who was overjoyed t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>When the three months were over Aladdin sent h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence\n",
       "117  Aladdin at last prevailed upon her to go befor...\n",
       "118  carry his request  She fetched a napkin and la...\n",
       "221  Besides this six slaves beautifully dressed to...\n",
       "48   treasure Aladdin forgot his fears and grasped ...\n",
       "334  amazed he could not say a word  Where is your ...\n",
       "209  and filled up the small house and garden  Alad...\n",
       "265  spokesman we cannot find jewels enough  The Su...\n",
       "457  deserve to be burnt to ashes but that this req...\n",
       "27   at nightfall to his mother who was overjoyed t...\n",
       "192  When the three months were over Aladdin sent h..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove punctuation from all sentences\n",
    "df['sentence'] = df['sentence'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Note: instead of regex a list of punctuation can be used, give it a try!\n",
    "punctuation = [\n",
    "    '.', ',', '!', '?', ':', ';', '\"', \"'\", '-', '_', '(', ')', '[', ']', '{', '}', '#', '@', '$', '%', '^', '&', '*',\n",
    "     '+', '=', '<', '>', '/', '\\\\', '|', '~', '`', '‚Äú', '‚Äù', '‚Äò', '‚Äô'\n",
    "]\n",
    "\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedee9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>the bed had been carried into some strange hou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>her father  his mother on hearing this burst o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>the princess that no man living would come up ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>returned aladdin  i wished your majesty to hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>hearing this said there is an old one on the c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>another such fearful night and wished to be se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>whom he murdered  he it was who put that wish ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>saying i must build a palace fit for her and t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>cellar and the princess put the powder aladdin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>him of her sons violent love for the princess ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence\n",
       "179  the bed had been carried into some strange hou...\n",
       "116  her father  his mother on hearing this burst o...\n",
       "198  the princess that no man living would come up ...\n",
       "261  returned aladdin  i wished your majesty to hav...\n",
       "303  hearing this said there is an old one on the c...\n",
       "189  another such fearful night and wished to be se...\n",
       "460  whom he murdered  he it was who put that wish ...\n",
       "229  saying i must build a palace fit for her and t...\n",
       "396  cellar and the princess put the powder aladdin...\n",
       "134  him of her sons violent love for the princess ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert all sentences to lowercase\n",
    "df['sentence'] = df['sentence'].str.lower()\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7bc800",
   "metadata": {},
   "source": [
    "**Sentences are a key unit of information when it comes to NLP** (as wells as tokens) in order to represent our data as a uniform \"block\" of text, we need to find out our longest sentence, the rest of them will later be padded with padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3fdce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length is 76\n"
     ]
    }
   ],
   "source": [
    "# Get the length of the longest senctence\n",
    "max_len = df['sentence'].str.len().max()\n",
    "print(f'max sentence length is {max_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e70bd",
   "metadata": {},
   "source": [
    "The transformers library provides a convenient way to load a variety of BERT models. Let's first load and explore a tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b2c34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa75445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size is: 30522\n"
     ]
    }
   ],
   "source": [
    "# Get the tokenizer vocabulary words\n",
    "vocab = bert_tokenizer.vocab\n",
    "vocab_size = len(vocab)\n",
    "print(f'vocab size is: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e209cb71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12204</th>\n",
       "      <td>encyclopedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2853</th>\n",
       "      <td>sold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3550</th>\n",
       "      <td>##ized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23141</th>\n",
       "      <td>##firmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13789</th>\n",
       "      <td>cheshire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2730</th>\n",
       "      <td>killed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12930</th>\n",
       "      <td>midst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15755</th>\n",
       "      <td>captains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21536</th>\n",
       "      <td>jing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>–≤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7216</th>\n",
       "      <td>comfort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2607</th>\n",
       "      <td>course</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15627</th>\n",
       "      <td>sprang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051</th>\n",
       "      <td>##lat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21737</th>\n",
       "      <td>##urne</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tokens\n",
       "12204  encyclopedia\n",
       "2853           sold\n",
       "3550         ##ized\n",
       "23141      ##firmed\n",
       "13789      cheshire\n",
       "2730         killed\n",
       "12930         midst\n",
       "15755      captains\n",
       "21536          jing\n",
       "1182              –≤\n",
       "7216        comfort\n",
       "2607         course\n",
       "15627        sprang\n",
       "20051         ##lat\n",
       "21737        ##urne"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the vocabulary words as a list, load them into a dataframe\n",
    "vocab_list = list(vocab.keys())\n",
    "vocab_df = pd.DataFrame(vocab_list, columns=['tokens'])\n",
    "vocab_df.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9146db8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 995 tokens that begin with \"unused\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>[unused662]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>[unused315]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>[unused776]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>[unused454]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>[unused754]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>[unused905]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>[unused549]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>[unused256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>[unused55]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>[unused717]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tokens\n",
       "667  [unused662]\n",
       "320  [unused315]\n",
       "781  [unused776]\n",
       "459  [unused454]\n",
       "759  [unused754]\n",
       "910  [unused905]\n",
       "554  [unused549]\n",
       "261  [unused256]\n",
       "56    [unused55]\n",
       "722  [unused717]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the count of tokens that begin with 'UNUSED'\n",
    "unused_tokens = vocab_df[vocab_df['tokens'].str.find('unused')>=0]\n",
    "print(f'There are {len(unused_tokens)} tokens that begin with \"unused\"')\n",
    "unused_tokens.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9effe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 997 tokens that have a size of 1 character\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>·¥∞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>Èï∑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>—à</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754</th>\n",
       "      <td>‰∫ï</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>„Äú</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>‡Æö</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>Â∞á</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1631</th>\n",
       "      <td>‚±º</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>Ê≥ï</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tokens\n",
       "1492      ·¥∞\n",
       "1967      Èï∑\n",
       "1203      —à\n",
       "1754      ‰∫ï\n",
       "1645      „Äú\n",
       "1383      ‡Æö\n",
       "1828      Â∞á\n",
       "1053      q\n",
       "1631      ‚±º\n",
       "1901      Ê≥ï"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the tokens that have a size of 1 character\n",
    "one_char_tokens = vocab_df[vocab_df['tokens'].str.len()==1]\n",
    "print(f'There are {len(one_char_tokens)} tokens that have a size of 1 character')\n",
    "one_char_tokens.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae4e532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 28042 tokens that likely reprensent English words\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20492</th>\n",
       "      <td>##mt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11114</th>\n",
       "      <td>gazed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16005</th>\n",
       "      <td>brewing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4433</th>\n",
       "      <td>draft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4099</th>\n",
       "      <td>enemy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9280</th>\n",
       "      <td>potentially</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11127</th>\n",
       "      <td>gravel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26613</th>\n",
       "      <td>klan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18187</th>\n",
       "      <td>jagged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17587</th>\n",
       "      <td>peterborough</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tokens\n",
       "20492          ##mt\n",
       "11114         gazed\n",
       "16005       brewing\n",
       "4433          draft\n",
       "4099          enemy\n",
       "9280    potentially\n",
       "11127        gravel\n",
       "26613          klan\n",
       "18187        jagged\n",
       "17587  peterborough"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the tokens which have a size of more than 2 characters and does not contain the word 'unused'\n",
    "two_char_tokens = vocab_df[(vocab_df['tokens'].str.len()>2) & (vocab_df['tokens'].str.find('unused')<0)]\n",
    "print(f'There are {len(two_char_tokens)} tokens that likely reprensent English words')\n",
    "two_char_tokens.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dc784c",
   "metadata": {},
   "source": [
    "Each sentence is currently represented as a list of characters. We need to transform this into a list of tokens, tokens then get converted into numbers using the tokenizers vocabulary as indexes. Here is an example with a phrase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826a1502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample sentence is:\n",
      "This is a sample sentence, which we will tokenize using the BERT tokenizer.\n",
      "\n",
      "The tokenized sentence is:\n",
      "['this', 'is', 'a', 'sample', 'sentence', ',', 'which', 'we', 'will', 'token', '##ize', 'using', 'the', 'bert', 'token', '##izer', '.']\n",
      "\n",
      "The numericalized sentence is:\n",
      "[2023, 2003, 1037, 7099, 6251, 1010, 2029, 2057, 2097, 19204, 4697, 2478, 1996, 14324, 19204, 17629, 1012]\n"
     ]
    }
   ],
   "source": [
    "# Example of tokenizing a sentence\n",
    "sample_sentence = \"This is a sample sentence, which we will tokenize using the BERT tokenizer.\"\n",
    "print(f'The sample sentence is:\\n{sample_sentence}')\n",
    "\n",
    "tokenized_sentence = bert_tokenizer.tokenize(sample_sentence)\n",
    "print(f'\\nThe tokenized sentence is:\\n{tokenized_sentence}')\n",
    "\n",
    "numericalized_sentence = bert_tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "print(f'\\nThe numericalized sentence is:\\n{numericalized_sentence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79392b66",
   "metadata": {},
   "source": [
    "We should now do the same for the sentences in the dataframe. Before proceding is a good idea to create a copy of what we have so far to be able to revert back to the original dataframe in case we need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205585c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the senctences dataframe\n",
    "tokens_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e1b1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tokenized_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>him of the lamp  he rubbed it and the genie ap...</td>\n",
       "      <td>[him, of, the, lamp, he, rubbed, it, and, the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>reality precious stones  he then asked for som...</td>\n",
       "      <td>[reality, precious, stones, he, then, asked, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>replied aladdin  so they sat at breakfast till...</td>\n",
       "      <td>[replied, ala, ##ddin, so, they, sat, at, brea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>himself in africa under the window of the prin...</td>\n",
       "      <td>[himself, in, africa, under, the, window, of, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>treasure aladdin forgot his fears and grasped ...</td>\n",
       "      <td>[treasure, ala, ##ddin, forgot, his, fears, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>the princess that no man living would come up ...</td>\n",
       "      <td>[the, princess, that, no, man, living, would, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>mine tell me what has become of an old lamp i ...</td>\n",
       "      <td>[mine, tell, me, what, has, become, of, an, ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>stood in a halfcircle round the throne with th...</td>\n",
       "      <td>[stood, in, a, half, ##ci, ##rcle, round, the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>hath made us aware of its virtues we will use ...</td>\n",
       "      <td>[hat, ##h, made, us, aware, of, its, virtues, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>people by her touch of their ailments whereupo...</td>\n",
       "      <td>[people, by, her, touch, of, their, ai, ##lm, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  \\\n",
       "155  him of the lamp  he rubbed it and the genie ap...   \n",
       "86   reality precious stones  he then asked for som...   \n",
       "97   replied aladdin  so they sat at breakfast till...   \n",
       "352  himself in africa under the window of the prin...   \n",
       "48   treasure aladdin forgot his fears and grasped ...   \n",
       "198  the princess that no man living would come up ...   \n",
       "369  mine tell me what has become of an old lamp i ...   \n",
       "214  stood in a halfcircle round the throne with th...   \n",
       "100  hath made us aware of its virtues we will use ...   \n",
       "432  people by her touch of their ailments whereupo...   \n",
       "\n",
       "                                    tokenized_sentence  \n",
       "155  [him, of, the, lamp, he, rubbed, it, and, the,...  \n",
       "86   [reality, precious, stones, he, then, asked, f...  \n",
       "97   [replied, ala, ##ddin, so, they, sat, at, brea...  \n",
       "352  [himself, in, africa, under, the, window, of, ...  \n",
       "48   [treasure, ala, ##ddin, forgot, his, fears, an...  \n",
       "198  [the, princess, that, no, man, living, would, ...  \n",
       "369  [mine, tell, me, what, has, become, of, an, ol...  \n",
       "214  [stood, in, a, half, ##ci, ##rcle, round, the,...  \n",
       "100  [hat, ##h, made, us, aware, of, its, virtues, ...  \n",
       "432  [people, by, her, touch, of, their, ai, ##lm, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize each sentence in the dataframe\n",
    "tokens_df['tokenized_sentence'] = tokens_df['sentence'].apply(bert_tokenizer.tokenize)\n",
    "tokens_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a113239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tokenized_sentence</th>\n",
       "      <th>numericalized_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>mother that though he consented to the marriag...</td>\n",
       "      <td>[mother, that, though, he, consent, ##ed, to, ...</td>\n",
       "      <td>[2388, 2008, 2295, 2002, 9619, 2098, 2000, 199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>this stone lies a treasure which is to be your...</td>\n",
       "      <td>[this, stone, lies, a, treasure, which, is, to...</td>\n",
       "      <td>[2023, 2962, 3658, 1037, 8813, 2029, 2003, 200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>may touch it so you must do exactly as i tell ...</td>\n",
       "      <td>[may, touch, it, so, you, must, do, exactly, a...</td>\n",
       "      <td>[2089, 3543, 2009, 2061, 2017, 2442, 2079, 359...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>reality precious stones  he then asked for som...</td>\n",
       "      <td>[reality, precious, stones, he, then, asked, f...</td>\n",
       "      <td>[4507, 9062, 6386, 2002, 2059, 2356, 2005, 207...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>and told his mother of his newly found uncle  ...</td>\n",
       "      <td>[and, told, his, mother, of, his, newly, found...</td>\n",
       "      <td>[1998, 2409, 2010, 2388, 1997, 2010, 4397, 217...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>aladdin looked up  she called to him to come t...</td>\n",
       "      <td>[ala, ##ddin, looked, up, she, called, to, him...</td>\n",
       "      <td>[21862, 18277, 2246, 2039, 2016, 2170, 2000, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>streets as usual a stranger asked him his age ...</td>\n",
       "      <td>[streets, as, usual, a, stranger, asked, him, ...</td>\n",
       "      <td>[4534, 2004, 5156, 1037, 7985, 2356, 2032, 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>cellar and the princess put the powder aladdin...</td>\n",
       "      <td>[cellar, and, the, princess, put, the, powder,...</td>\n",
       "      <td>[15423, 1998, 1996, 4615, 2404, 1996, 9898, 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>not but he will use violence  aladdin comforte...</td>\n",
       "      <td>[not, but, he, will, use, violence, ala, ##ddi...</td>\n",
       "      <td>[2025, 2021, 2002, 2097, 2224, 4808, 21862, 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>said your father had a brother but i always th...</td>\n",
       "      <td>[said, your, father, had, a, brother, but, i, ...</td>\n",
       "      <td>[2056, 2115, 2269, 2018, 1037, 2567, 2021, 104...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  \\\n",
       "146  mother that though he consented to the marriag...   \n",
       "46   this stone lies a treasure which is to be your...   \n",
       "47   may touch it so you must do exactly as i tell ...   \n",
       "86   reality precious stones  he then asked for som...   \n",
       "15   and told his mother of his newly found uncle  ...   \n",
       "365  aladdin looked up  she called to him to come t...   \n",
       "9    streets as usual a stranger asked him his age ...   \n",
       "396  cellar and the princess put the powder aladdin...   \n",
       "379  not but he will use violence  aladdin comforte...   \n",
       "16   said your father had a brother but i always th...   \n",
       "\n",
       "                                    tokenized_sentence  \\\n",
       "146  [mother, that, though, he, consent, ##ed, to, ...   \n",
       "46   [this, stone, lies, a, treasure, which, is, to...   \n",
       "47   [may, touch, it, so, you, must, do, exactly, a...   \n",
       "86   [reality, precious, stones, he, then, asked, f...   \n",
       "15   [and, told, his, mother, of, his, newly, found...   \n",
       "365  [ala, ##ddin, looked, up, she, called, to, him...   \n",
       "9    [streets, as, usual, a, stranger, asked, him, ...   \n",
       "396  [cellar, and, the, princess, put, the, powder,...   \n",
       "379  [not, but, he, will, use, violence, ala, ##ddi...   \n",
       "16   [said, your, father, had, a, brother, but, i, ...   \n",
       "\n",
       "                                numericalized_sentence  \n",
       "146  [2388, 2008, 2295, 2002, 9619, 2098, 2000, 199...  \n",
       "46   [2023, 2962, 3658, 1037, 8813, 2029, 2003, 200...  \n",
       "47   [2089, 3543, 2009, 2061, 2017, 2442, 2079, 359...  \n",
       "86   [4507, 9062, 6386, 2002, 2059, 2356, 2005, 207...  \n",
       "15   [1998, 2409, 2010, 2388, 1997, 2010, 4397, 217...  \n",
       "365  [21862, 18277, 2246, 2039, 2016, 2170, 2000, 2...  \n",
       "9    [4534, 2004, 5156, 1037, 7985, 2356, 2032, 201...  \n",
       "396  [15423, 1998, 1996, 4615, 2404, 1996, 9898, 21...  \n",
       "379  [2025, 2021, 2002, 2097, 2224, 4808, 21862, 18...  \n",
       "16   [2056, 2115, 2269, 2018, 1037, 2567, 2021, 104...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add the numericalized sentences to the dataframe\n",
    "tokens_df['numericalized_sentence'] = tokens_df['tokenized_sentence'].apply(bert_tokenizer.convert_tokens_to_ids)\n",
    "tokens_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8de28c0",
   "metadata": {},
   "source": [
    "Phrases that will be inputted to a BERT model must include the special tokens `[CLS]` and `[SEP]`. These tokens are used to indicate the start and end of the input sequence. Let's add these tokens to the sample phrase. Another special token is `[PAD]`, which is used to pad shorter sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c621a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The tokenized sentence is:\n",
      "['CLS', 'this', 'is', 'a', 'sample', 'sentence', ',', 'which', 'we', 'will', 'token', '##ize', 'using', 'the', 'bert', 'token', '##izer', '.', 'SEP']\n",
      "\n",
      "The numericalized sentence is:\n",
      "[100, 2023, 2003, 1037, 7099, 6251, 1010, 2029, 2057, 2097, 19204, 4697, 2478, 1996, 14324, 19204, 17629, 1012, 100]\n",
      "- The token ID for the special token [CLS] is: 101\n",
      "- The token ID for the special token [SEP] is: 102\n",
      "- The token ID for the special token [PAD] is: 0\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence = ['CLS'] + tokenized_sentence + ['SEP']\n",
    "print(f'\\nThe tokenized sentence is:\\n{tokenized_sentence}')\n",
    "\n",
    "numericalized_sentence = bert_tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "print(f'\\nThe numericalized sentence is:\\n{numericalized_sentence}')\n",
    "\n",
    "# Print the IDs for the special tokens for the BERT model\n",
    "print(f'- The token ID for the special token [CLS] is: {bert_tokenizer.cls_token_id}')\n",
    "print(f'- The token ID for the special token [SEP] is: {bert_tokenizer.sep_token_id}')\n",
    "print(f'- The token ID for the special token [PAD] is: {bert_tokenizer.pad_token_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176fd665",
   "metadata": {},
   "source": [
    "As the exampled indicates, we need to add the [CLS] and [SEP] tokens and tokenize each sentence of the text dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd7f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20     [101, 2022, 4527, 2012, 2025, 2383, 2464, 2032...\n",
       "449    [101, 5689, 2013, 1996, 8514, 2065, 2008, 2003...\n",
       "475    [101, 2005, 2116, 2086, 2975, 2369, 2032, 1037...\n",
       "420    [101, 2062, 10433, 1998, 2062, 23626, 2084, 23...\n",
       "149    [101, 21862, 18277, 4741, 19080, 2005, 3053, 2...\n",
       "112    [101, 2004, 2016, 2253, 1999, 1998, 2246, 2061...\n",
       "388    [101, 2187, 2014, 9140, 2098, 2841, 18576, 210...\n",
       "61     [101, 2677, 1997, 1996, 5430, 1996, 16669, 663...\n",
       "10     [101, 1996, 2365, 1997, 2442, 9331, 3270, 1996...\n",
       "81     [101, 8116, 2033, 2013, 2023, 2173, 26090, 199...\n",
       "Name: numericalized_sentence, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add the 100 special tokens to the numericalized sentences on the dataframe\n",
    "tokens_df['numericalized_sentence'] = tokens_df['numericalized_sentence'].apply(lambda x: [bert_tokenizer.cls_token_id] + x + [bert_tokenizer.sep_token_id])\n",
    "tokens_df['numericalized_sentence'].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b1d86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the 0 padding to the numericalized sentences on the dataframe\n",
    "tokens_df['numericalized_sentence'] = tokens_df['numericalized_sentence'].apply(lambda x: x + [bert_tokenizer.pad_token_id] * (max_len - len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1360e61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "348    76\n",
       "343    76\n",
       "120    76\n",
       "144    76\n",
       "201    76\n",
       "381    76\n",
       "18     76\n",
       "439    76\n",
       "77     76\n",
       "339    76\n",
       "Name: numericalized_sentence_length, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add a new column that indicates the length of the numericalized sentences\n",
    "tokens_df['numericalized_sentence_length'] = tokens_df['numericalized_sentence'].apply(len)\n",
    "tokens_df['numericalized_sentence_length'] .sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1ed282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(447,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract the numericalized sentences from the dataframe\n",
    "numericalized_sentences = tokens_df['numericalized_sentence'].values\n",
    "numericalized_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2344b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each row of the numericalized sentences to a list\n",
    "numericalized_sentences = [list(x) for x in numericalized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeed5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the numericalized sentences is: (447, 76)\n",
      "[[  101 21862 18277 ...     0     0     0]\n",
      " [  101  2045  2320 ...     0     0     0]\n",
      " [  101  1037 23358 ...     0     0     0]\n",
      " ...\n",
      " [  101  2044  2023 ...     0     0     0]\n",
      " [  101  2002  4594 ...     0     0     0]\n",
      " [  101  2005  2116 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# Convert the list into a 2D NumPy array\n",
    "numericalized_sentences = np.array(numericalized_sentences)\n",
    "print(f'The shape of the numericalized sentences is: {numericalized_sentences.shape}')\n",
    "print(numericalized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465072a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101, 21862, 18277,  ...,     0,     0,     0],\n",
      "        [  101,  2045,  2320,  ...,     0,     0,     0],\n",
      "        [  101,  1037, 23358,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2044,  2023,  ...,     0,     0,     0],\n",
      "        [  101,  2002,  4594,  ...,     0,     0,     0],\n",
      "        [  101,  2005,  2116,  ...,     0,     0,     0]], dtype=torch.int32)\n",
      "the shape of the numericalized tensor is: torch.Size([447, 76])\n"
     ]
    }
   ],
   "source": [
    "#  Convert the numpy array into a Tensor\n",
    "numericalized_sentences = torch.from_numpy(numericalized_sentences)\n",
    "print(numericalized_sentences)\n",
    "print(f'the shape of the numericalized tensor is: {numericalized_sentences.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bbc913",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d321c2b",
   "metadata": {},
   "source": [
    "## Part 2: How to Structure Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7af91a",
   "metadata": {},
   "source": [
    "Since this problem is so diverse, it can be hard to look at. and turn it all into a 1D object, or list. \n",
    "\n",
    "If we can wrap our heads around this, we can take a soup of texts from literature, websites, reviews, almost anything imaginable, and wipe it clean so it can be tokenized. BERT recognizes this problem and doesn't rely on the text being structured in order to work. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25cb336",
   "metadata": {},
   "source": [
    "### Flattening the structure, starting from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebc42be",
   "metadata": {},
   "source": [
    "The most simple approach, or the lowest common denominator is to remove the structure of the data, and turn it into a 1D Object, either as a native Python String Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa894d79",
   "metadata": {},
   "source": [
    "### File Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d97ed0",
   "metadata": {},
   "source": [
    "## .TXT Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302f1dfa",
   "metadata": {},
   "source": [
    "The simplest method to split text data is to use the .split() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe98eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aefc0628",
   "metadata": {},
   "source": [
    "## CSV "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c0aca4",
   "metadata": {},
   "source": [
    "CSV files are organized by columns, so it makes it relatively straightforward to index the text columns. \n",
    "\n",
    "Similarly to how a numpy array can be flattened, it is not to difficult to do this and flatten it all into one string\n",
    "The concatenate method of a pandas series can do this\n",
    "\n",
    ".str.cat(sep = ' ')\n",
    ".str.cat(sep = '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00bda3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hskVocabPath = DATA_DIR/'ChineseVocabulary'/'HSK Official With Definitions 2012 L3 freqorder.txt'\n",
    "\n",
    "hskVocab = pd.read_csv(hskVocabPath,\n",
    "                       header = None,\n",
    "                       index_col = None,\n",
    "                       sep='\\t'\n",
    "                      )\n",
    "\n",
    "hskVocabColumns = ['Simplified','Traditional','Pinyin_Numeric','Pinyin_Accented','Definition']\n",
    "hskVocab.columns = hskVocabColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ee0e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "hanzi = hskVocab['Simplified'].str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60b043fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Âïä Ëøò Êää Ëøá Â¶ÇÊûú Âè™ Ë¢´ Ë∑ü Ëá™Â∑± Áî® ÂÉè ‰∏∫ ÈúÄË¶Å Â∫îËØ• Ëµ∑Êù• Êâç Âèà Êãø Êõ¥ Â∏¶ ÁÑ∂Âêé ‰∏ÄÊ†∑ ÂΩìÁÑ∂ Áõ∏‰ø° ËÆ§‰∏∫ ÊòéÁôΩ ‰∏ÄÁõ¥ Âú∞ Âú∞Êñπ Á¶ªÂºÄ ‰∏ÄÂÆö ËøòÊòØ Âèë ÂèëÁé∞ ËÄå‰∏î ÂøÖÈ°ª Êîæ ‰∏∫‰∫Ü Âêë ËÄÅ ‰Ωç ÂÖà Áßç ÊúÄÂêé ÂÖ∂‰ªñ ËÆ∞Âæó ÊàñËÄÖ ËøáÂéª ÊãÖÂøÉ Êù° ‰ª•Ââç Èïø ‰∏ñÁïå ÈáçË¶Å Âà´‰∫∫ Êú∫‰ºö Âº† Êé• ÊØîËµõ \n"
     ]
    }
   ],
   "source": [
    "print(hanzi[0:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b618a784",
   "metadata": {},
   "outputs": [],
   "source": [
    "definitions = hskVocab['Definition'].str.cat(sep='. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34bcbb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ah; (particle showing elation, doubt, puzzled surprise, or approval). still; yet; in addition; even | repay; to return. (mw for things with handles); \n"
     ]
    }
   ],
   "source": [
    "print(definitions[0:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2a62d5",
   "metadata": {},
   "source": [
    "It is also possible to join columns together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "935721e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Âïä ah; (particle showing elation, doubt, puzzle...\n",
       "1    Ëøò still; yet; in addition; even | repay; to re...\n",
       "2    Êää (mw for things with handles); (pretransitive...\n",
       "3    Ëøá to pass; to cross; go over; (indicates a pas...\n",
       "4                             Â¶ÇÊûú if; in the event that\n",
       "Name: Simplified, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hanziWithDefinitions = hskVocab['Simplified'].str.cat(\n",
    "    hskVocab['Definition'],sep=' ')\n",
    "hanziWithDefinitions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f220ff",
   "metadata": {},
   "source": [
    "It is also to concatenate rows together, if you would like to preserve a relationship when they get tokenized in the same bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51cc019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hanziWithDefinitions = hanziWithDefinitions.str.cat(sep='. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "efda2a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Âïä ah; (particle showing elation, doubt, puzzled surprise, or approval). Ëøò still; yet; in addition; \n"
     ]
    }
   ],
   "source": [
    "print(hanziWithDefinitions[0:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcde2ac",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9778241f",
   "metadata": {},
   "source": [
    "Json files have a different paradigm than csv files which are table based. As a result, the results of this may vary depending on the structure. \n",
    "\n",
    "In this course, we look to cover key threshold concepts well, while being careful not to get over extended. Since tensors are dependent on unique indices and having regular shapes, this course gets into more detail about tables than it does about JSON. \n",
    "\n",
    "With this in mind, there are two things to consider about JSON files. Json files do not have to have a regular structure. One JSON cell can contain as few or as many items as required. Json files can also be heavily nested in ways that don't play well with the paradigm of tensors.\n",
    "\n",
    "If a JSON file has a regular structure, then it will fit easier into a dataframe, such as for an API request. If it does not have a regular structure, like a configuration file, then it will probably not transfer over well into a dataframe.\n",
    "\n",
    "In this case, it can be treated as a Series instead, since it is not bound by the same requirements. \n",
    "\n",
    "In many cases, it may be necessary to unpack it using python's JSON library. \n",
    "\n",
    "https://www.marsja.se/how-to-read-and-write-json-files-using-python-and-pandas/\n",
    "\n",
    "\n",
    "If you end up with some highly nested JSON objects, this tutorial goes through some pretty complicated data and might give some inspiration.\n",
    "https://medium.com/analytics-vidhya/extract-the-useful-data-from-jason-file-for-data-sceince-34ed5ae0b350\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5cb57200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sub_ID': ['1', '2', '3', '4', '5', '6', '7', '8'], 'Name': ['Erik', 'Daniel', 'Michael', 'Sven', 'Gary', 'Carol', 'Lisa', 'Elisabeth'], 'Salary': ['723.3', '515.2', '621', '731', '844.15', '558', '642.8', '732.5'], 'StartDate': ['1/1/2011', '7/23/2013', '12/15/2011', '6/11/2013', '3/27/2011', '5/21/2012', '7/30/2013', '6/17/2014'], 'Department': ['IT', 'Manegement', 'IT', 'HR', 'Finance', 'IT', 'Manegement', 'IT'], 'Sex': ['M', 'M', 'M', 'M', 'M', 'F', 'F', 'F']}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Creating a Python Dictionary\n",
    "data = {\"Sub_ID\":[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\" ],\n",
    "        \"Name\":[\"Erik\", \"Daniel\", \"Michael\", \"Sven\",\n",
    "                \"Gary\", \"Carol\",\"Lisa\", \"Elisabeth\" ],\n",
    "        \"Salary\":[\"723.3\", \"515.2\", \"621\", \"731\", \n",
    "                  \"844.15\",\"558\", \"642.8\", \"732.5\" ],\n",
    "        \"StartDate\":[ \"1/1/2011\", \"7/23/2013\", \"12/15/2011\",\n",
    "                     \"6/11/2013\", \"3/27/2011\",\"5/21/2012\", \n",
    "                     \"7/30/2013\", \"6/17/2014\"],\n",
    "        \"Department\":[ \"IT\", \"Manegement\", \"IT\", \"HR\", \n",
    "                      \"Finance\", \"IT\", \"Manegement\", \"IT\"],\n",
    "        \"Sex\":[ \"M\", \"M\", \"M\", \n",
    "              \"M\", \"M\", \"F\", \"F\", \"F\"]}\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ff1d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Parse JSON\n",
    "with open('data.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c949b4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sub_ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Salary</th>\n",
       "      <th>StartDate</th>\n",
       "      <th>Department</th>\n",
       "      <th>Sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Erik</td>\n",
       "      <td>723.30</td>\n",
       "      <td>1/1/2011</td>\n",
       "      <td>IT</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Daniel</td>\n",
       "      <td>515.20</td>\n",
       "      <td>7/23/2013</td>\n",
       "      <td>Manegement</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Michael</td>\n",
       "      <td>621.00</td>\n",
       "      <td>12/15/2011</td>\n",
       "      <td>IT</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Sven</td>\n",
       "      <td>731.00</td>\n",
       "      <td>6/11/2013</td>\n",
       "      <td>HR</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Gary</td>\n",
       "      <td>844.15</td>\n",
       "      <td>3/27/2011</td>\n",
       "      <td>Finance</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sub_ID     Name  Salary   StartDate  Department Sex\n",
       "0       1     Erik  723.30    1/1/2011          IT   M\n",
       "1       2   Daniel  515.20   7/23/2013  Manegement   M\n",
       "2       3  Michael  621.00  12/15/2011          IT   M\n",
       "3       4     Sven  731.00   6/11/2013          HR   M\n",
       "4       5     Gary  844.15   3/27/2011     Finance   M"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Read JSON as a dataframe with Pandas:\n",
    "json_df = pd.read_json('data.json')\n",
    "json_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a42561ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Erik IT. Daniel Manegement. Michael IT. Sven HR. Gary Finance. Carol IT. Lisa Manegement. Elisabeth IT'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(json_df['Name'] + ' ' + json_df['Department']).str.cat(sep='. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e385c9",
   "metadata": {},
   "source": [
    "## PDF and Word Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96e8c6f",
   "metadata": {},
   "source": [
    "PDFs and word documents contain information which makes it more readable than a plain .txt file. Word Documents are more regularly structured than a PDF, which is purely designed to be viewed by people. \n",
    "\n",
    "For this, PDF miner can be used to extract PDF documents, while python Docx can be used to extract text from a word document. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2b306d",
   "metadata": {},
   "source": [
    "### Docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d448520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install docx -qq\n",
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0bf79161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<docx.text.paragraph.Paragraph at 0x7fc929707a90>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = docx.Document()\n",
    "\n",
    "document.add_heading('HSK Vocabulary Definitions')\n",
    "\n",
    "document.add_paragraph(hanziWithDefinitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fc2ee26f",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    " \n",
    "# open connection to Word Document\n",
    "#doc = docx.Document(\"FILE_PATH\")\n",
    " \n",
    "# read in each paragraph in file\n",
    "rawText = [p.text for p in document.paragraphs]\n",
    "\n",
    "# Since there are multiple paragraphs, they need to\n",
    "# concatenated\n",
    "rawText = ''.join(rawText)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "10e3cd95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HSK Vocabulary DefinitionsÂïä ah; (particle showing elation, doubt, puzzled surprise, or approval). Ëøò still; yet; in addition; even | repay; to return. '"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawText[:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb814b5",
   "metadata": {},
   "source": [
    "# Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467c9431",
   "metadata": {},
   "source": [
    "### Discuss the following:\n",
    "* What was the pipeline of this exercise?\n",
    "* Please give a summary of the data cleaning and preprocessing steps.\n",
    "* What is the difference between a token and a sentence?\n",
    "* Why did we converted the tokens to numbers?\n",
    "* Why did we add the special tokens?\n",
    "* What advantages are offered by Pandas for text manipulation?\n",
    "* Would this approach be suitable for complex datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d52596f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise:\n",
    "\n",
    "* Repeat this pipeline with 3 different books that appear very different in nature (don't add the special tokens).\n",
    "* When you obtain the numericalized sentences, convert them into a long 1D Numpy array.\n",
    "* Plot the distribution of the numericalized tokens for each book using histograms.\n",
    "* Comment your experience during the next lesson."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b35736f271ae3a6547f08ef3ad12296102f5e6031b2a7c6493a7e5cc9f313275"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
