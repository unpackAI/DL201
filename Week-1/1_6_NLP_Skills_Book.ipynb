{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b990127e",
   "metadata": {},
   "source": [
    "## ðŸ’» UnpackAI DL201 Bootcamp - Week 1 - Skills: NLP\n",
    "\n",
    "### ðŸ“• Learning Objectives\n",
    "\n",
    "* Solidify the basic notion of NLP and how it can be applied to a variety of tasks.\n",
    "* Practice using Pandas for loading and processing text data.\n",
    "* Ilustrate the process of converting a text document into a dataframe and from there into a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2717a57",
   "metadata": {},
   "source": [
    "### A basic NLP Overview\n",
    "\n",
    "From Wikipedia:\n",
    "- \"Natural language processing (NLP) is a subfield of **linguistics, computer science, and artificial intelligence** concerned with the interactions between computers and **human language**, in particular how to program computers to **process and analyze** large amounts of natural language data. The goal is a computer capable of **\"understanding\"** the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\"\n",
    "\n",
    "- Approaches to NLP tasks:\n",
    "    - Rule-based\n",
    "    - Traditional machine learning\n",
    "    - Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a04c7e",
   "metadata": {},
   "source": [
    "In NLP, we often need to perform text preprocessing, such as removing stop words, stemming, lemmatization, and tokenization.\n",
    "A nice overview is presented in: \n",
    "- https://stanfordnlp.github.io/CoreNLP/ \n",
    "- https://www.techtarget.com/searchenterpriseai/definition/natural-language-processing-NLP\n",
    "\n",
    "Common NLP tasks:\n",
    "- Classification\n",
    "- Masked filing\n",
    "- Text prediction\n",
    "- Sentiment analysis\n",
    "    - Positive\n",
    "    - Negative\n",
    "    - Subjectivity\n",
    "- Entity recognition\n",
    "    - Person\n",
    "    - Location\n",
    "    - Organization\n",
    "- Entity extraction\n",
    "- Keyword extraction\n",
    "- Topic extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f71e40e",
   "metadata": {},
   "source": [
    "### Ilustrative example\n",
    "\n",
    "Below there is a code example that that illustrates the usage of Pandas for text manipulation and a few exploratory steps to create Tensors representing the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9dbbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "#! pip install transformers\n",
    "#!pip install openpyxl\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe83bb",
   "metadata": {},
   "source": [
    "It is important to set correctly your data folder path as a local variable, depending on where you run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91be4bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data directory path as a variable\n",
    "\n",
    "# Uncomment this for Kaggle\n",
    "#!git clone https://github.com/unpackAI/DL201.git\n",
    "#DATA_DIR = Path('/kaggle/working/DL201/data/nlp') #uncomment for kaggle\n",
    "\n",
    "# Uncomment this for local\n",
    "os.chdir('../data/nlp')\n",
    "DATA_DIR = os.getcwd()\n",
    "\n",
    "print(f'data directory is {DATA_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfe8f1d",
   "metadata": {},
   "source": [
    "Let's load a sample text file and feed it into the BERT model. The data/nlp folder of the repository contains a txt file with sentences from a book, the book was taken from: http://www.textfiles.com/stories/. Feel free to download a different book and use it when explolring this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5340ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample text, from the data folder\n",
    "os.chdir(DATA_DIR)\n",
    "sample_text = open('alad10.txt').read()\n",
    "\n",
    "# Split the text into sentences\n",
    "sentences = sample_text.split('\\n')\n",
    "\n",
    "# Load the sentences into a dataframe\n",
    "df = pd.DataFrame(sentences, columns=['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8f56a9",
   "metadata": {},
   "source": [
    "As it has been reitared before, loading the data into Pandas gives us tremendous flexibility to perform data cleaning and preprocessing with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c3329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect some of the sentences\n",
    "df.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f87549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation from all sentences\n",
    "df['sentence'] = df['sentence'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Note: instead of regex a list of punctuation can be used, give it a try!\n",
    "punctuation = [\n",
    "    '.', ',', '!', '?', ':', ';', '\"', \"'\", '-', '_', '(', ')', '[', ']', '{', '}', '#', '@', '$', '%', '^', '&', '*',\n",
    "     '+', '=', '<', '>', '/', '\\\\', '|', '~', '`', 'â€œ', 'â€', 'â€˜', 'â€™'\n",
    "]\n",
    "\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedee9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all sentences to lowercase\n",
    "df['sentence'] = df['sentence'].str.lower()\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7bc800",
   "metadata": {},
   "source": [
    "**Sentences are a key unit of information when it comes to NLP** (as wells as tokens) in order to represent our data as a uniform \"block\" of text, we need to find out our longest sentence, the rest of them will later be padded with padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3fdce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the length of the longest senctence\n",
    "max_len = df['sentence'].str.len().max()\n",
    "print(f'max sentence length is {max_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e70bd",
   "metadata": {},
   "source": [
    "The transformers library provides a convenient way to load a variety of BERT models. Let's first load and explore a tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b2c34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa75445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tokenizer vocabulary words\n",
    "vocab = bert_tokenizer.vocab\n",
    "vocab_size = len(vocab)\n",
    "print(f'vocab size is: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e209cb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vocabulary words as a list, load them into a dataframe\n",
    "vocab_list = list(vocab.keys())\n",
    "vocab_df = pd.DataFrame(vocab_list, columns=['tokens'])\n",
    "vocab_df.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9146db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the count of tokens that begin with 'UNUSED'\n",
    "unused_tokens = vocab_df[vocab_df['tokens'].str.find('unused')>=0]\n",
    "print(f'There are {len(unused_tokens)} tokens that begin with \"unused\"')\n",
    "unused_tokens.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tokens that have a size of 1 character\n",
    "one_char_tokens = vocab_df[vocab_df['tokens'].str.len()==1]\n",
    "print(f'There are {len(one_char_tokens)} tokens that have a size of 1 character')\n",
    "one_char_tokens.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae4e532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tokens which have a size of more than 2 characters and does not contain the word 'unused'\n",
    "two_char_tokens = vocab_df[(vocab_df['tokens'].str.len()>2) & (vocab_df['tokens'].str.find('unused')<0)]\n",
    "print(f'There are {len(two_char_tokens)} tokens that likely reprensent English words')\n",
    "two_char_tokens.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16299a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unpackAIdev",
   "language": "python",
   "name": "unpackaidev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
