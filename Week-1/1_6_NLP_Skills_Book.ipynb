{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a244d1",
   "metadata": {},
   "source": [
    "# üíª UnpackAI DL201 Bootcamp - Week 1 - Skills: NLP\n",
    "\n",
    "## üìï Learning Objectives\n",
    "\n",
    "* Gain an appreciation for how NLP models accept data\n",
    "* To see past the complexity and extract text from common file formats\n",
    "* Perform cursory EDA by using a Pandas Series while expanding confidence and awareness of it's capabilities\n",
    "\n",
    "## üìñ Concepts map\n",
    "* Structured vs. Unstructured Data\n",
    "* Qualitative vs Quantitative Data\n",
    "* Flattening Data Structures\n",
    "* Tokenization\n",
    "* Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438353f",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Why is Natural Language Processing (NLP) Data Notorious?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720283fe",
   "metadata": {},
   "source": [
    "The nature of text data makes it intimidating compared to other forms of data. It is not too difficult to consider an image or dataframe because we are familiar with these from everyday life. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0b2c9f",
   "metadata": {},
   "source": [
    "## Text is Unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d633438",
   "metadata": {},
   "source": [
    "Instead, Text data can take endless forms because it represents ideas. As a result, text and words, are very free. They can be as short as an utterance, to as long as a dictionary. They can be stored as scanned PDFs of archives written with typewriters, word documents, e-books, front end web pages, back end API responses, or even just plain .txt files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62355f23",
   "metadata": {},
   "source": [
    "## Text is Qualitative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450c85f2",
   "metadata": {},
   "source": [
    "Text doesn't have the meaning of numbers, and can be interpreted in different ways. \n",
    "\n",
    "For example, a mountain is normally a noun, but it is an adjective in mountain lion, and a mountain lion is also called a puma or cougar. There is inherent ambiguity in language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c615d93a",
   "metadata": {},
   "source": [
    "# Part 0 : Code preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad5880b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers openpyxl docx -qq\n",
    "!git clone https://github.com/unpackAI/DL201.git\n",
    "\n",
    "\n",
    "# Imports \n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import requests\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "#Kaggle config\n",
    "DATA_DIR = Path('/kaggle/working/DL201/data') #uncomment for kaggle\n",
    "IMAGE_DIR = Path('/kaggle/working/DL201/img') #Uncomment for Kaggle\n",
    "\n",
    "\n",
    "# Local Config\n",
    "#week_1_folder = os.getcwd()\n",
    "#os.chdir(\"..\")\n",
    "#DATA_DIR = os.getcwd() + '/data'\n",
    "#IMAGE_DIR = os.getcwd() + '/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8156e9d",
   "metadata": {},
   "source": [
    "# Part 1: How NLP Quantifies Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2717a57",
   "metadata": {},
   "source": [
    "### A basic NLP Overview\n",
    "\n",
    "From Wikipedia:\n",
    "- \"Natural language processing (NLP) is a subfield of **linguistics, computer science, and artificial intelligence** concerned with the interactions between computers and **human language**, in particular how to program computers to **process and analyze** large amounts of natural language data. The goal is a computer capable of **\"understanding\"** the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\"\n",
    "\n",
    "- Approaches to NLP tasks:\n",
    "    - Rule-based\n",
    "    - Traditional machine learning\n",
    "    - Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a04c7e",
   "metadata": {},
   "source": [
    "In NLP, we often need to perform text preprocessing, such as removing stop words, stemming, lemmatization, and tokenization.\n",
    "A nice overview is presented in: \n",
    "- https://stanfordnlp.github.io/CoreNLP/ \n",
    "- https://www.techtarget.com/searchenterpriseai/definition/natural-language-processing-NLP\n",
    "\n",
    "Common NLP tasks:\n",
    "- Classification\n",
    "- Masked filing\n",
    "- Text prediction\n",
    "- Sentiment analysis\n",
    "    - Positive\n",
    "    - Negative\n",
    "    - Subjectivity\n",
    "- Entity recognition\n",
    "    - Person\n",
    "    - Location\n",
    "    - Organization\n",
    "- Entity extraction\n",
    "- Keyword extraction\n",
    "- Topic extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f71e40e",
   "metadata": {},
   "source": [
    "### Ilustrative example\n",
    "\n",
    "Below there is a code example that that illustrates the usage of Pandas for text manipulation and a few exploratory steps to create Tensors representing the text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfe8f1d",
   "metadata": {},
   "source": [
    "Let's load a sample book conveniently available in txt format from the collection at http://www.textfiles.com/stories/ the book in this case is Aladdin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5340ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample text, from the provided url\n",
    "response = requests.get('http://www.textfiles.com/stories/alad10.txt')\n",
    "sample_text = response.text\n",
    "\n",
    "# Split the text into sentences\n",
    "sentences = sample_text.split('\\n')\n",
    "\n",
    "# Load the sentences into a dataframe\n",
    "df = pd.DataFrame(sentences, columns=['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8f56a9",
   "metadata": {},
   "source": [
    "As it has been reitared before, loading the data into Pandas gives us tremendous flexibility to perform data cleaning and preprocessing with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a8c3329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>and requesting that the holy Fatima should be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Aladdin looked up.  She called to him to come ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Aladdin, who was overwhelmed at first, but pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\"but he died a long while ago.\"  On this the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>found himself outside.  As soon as his eyes co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>daughter?\" demanded the Sultan.  \"For the firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>sign she was reconciled to him.  Before drinki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>invited you to sup with me; but I am tired of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>and two bottles of wine.  Aladdin's mother, wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>with smiles, leading him to believe that you h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>rubies, diamonds and emeralds, he cried, \"It i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>called Fatima, thinking she might be of use to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>into a basket, and went to the palace, crying:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>the bath, and peeped through a chink.  The Pri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence\n",
       "465  and requesting that the holy Fatima should be ...\n",
       "365  Aladdin looked up.  She called to him to come ...\n",
       "154  Aladdin, who was overwhelmed at first, but pre...\n",
       "11   \"but he died a long while ago.\"  On this the s...\n",
       "82   found himself outside.  As soon as his eyes co...\n",
       "335  daughter?\" demanded the Sultan.  \"For the firs...\n",
       "399  sign she was reconciled to him.  Before drinki...\n",
       "394  invited you to sup with me; but I am tired of ...\n",
       "95   and two bottles of wine.  Aladdin's mother, wh...\n",
       "384  with smiles, leading him to believe that you h...\n",
       "258  rubies, diamonds and emeralds, he cried, \"It i...\n",
       "422  called Fatima, thinking she might be of use to...\n",
       "297  into a basket, and went to the palace, crying:...\n",
       "191                                                 \\r\n",
       "111  the bath, and peeped through a chink.  The Pri..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect some of the sentences\n",
    "df.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19f93c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the sentences that have less than 3 words\n",
    "df = df[df['sentence'].str.split().str.len() > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8f87549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenis\\AppData\\Local\\Temp\\ipykernel_16856\\2308473420.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['sentence'] = df['sentence'].str.replace('[^\\w\\s]','')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>After this Aladdin and his wife lived in peace\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>him in his childhood knew him not he had grown...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>him what he thought of it  It is truly beautif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>passed there Her mother did not believe her in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>received him in the hall of the fourandtwenty ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>deserve to be burnt to ashes but that this req...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the streets with little idle boys like himself...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>surprised to receive his jewels again and visi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>from you but from the brother of the African m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>all but the Vizier and bade her speak freely p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence\n",
       "473   After this Aladdin and his wife lived in peace\\r\n",
       "225  him in his childhood knew him not he had grown...\n",
       "439  him what he thought of it  It is truly beautif...\n",
       "180  passed there Her mother did not believe her in...\n",
       "413  received him in the hall of the fourandtwenty ...\n",
       "457  deserve to be burnt to ashes but that this req...\n",
       "6    the streets with little idle boys like himself...\n",
       "270  surprised to receive his jewels again and visi...\n",
       "458  from you but from the brother of the African m...\n",
       "132  all but the Vizier and bade her speak freely p..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation from all sentences\n",
    "df['sentence'] = df['sentence'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Note: instead of regex a list of punctuation can be used, give it a try!\n",
    "punctuation = [\n",
    "    '.', ',', '!', '?', ':', ';', '\"', \"'\", '-', '_', '(', ')', '[', ']', '{', '}', '#', '@', '$', '%', '^', '&', '*',\n",
    "     '+', '=', '<', '>', '/', '\\\\', '|', '~', '`', '‚Äú', '‚Äù', '‚Äò', '‚Äô'\n",
    "]\n",
    "\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aedee9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>showed him the window finished  the sultan emb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>his finger and gave it to aladdin bidding him ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>humour  he begged to know what was amiss and s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>and will obey thee in all things  aladdin fear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>in it back to china  this was done and the pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>bidding him make haste  but aladdin first call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>which i shall always wear on my finger  when t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>hanging from the dome  if that is all replied ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>put him outside in the cold and return at dayb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>to you  the princess was too frightened to spe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence\n",
       "271  showed him the window finished  the sultan emb...\n",
       "57   his finger and gave it to aladdin bidding him ...\n",
       "447  humour  he begged to know what was amiss and s...\n",
       "80   and will obey thee in all things  aladdin fear...\n",
       "407  in it back to china  this was done and the pri...\n",
       "218  bidding him make haste  but aladdin first call...\n",
       "101  which i shall always wear on my finger  when t...\n",
       "449  hanging from the dome  if that is all replied ...\n",
       "163  put him outside in the cold and return at dayb...\n",
       "167  to you  the princess was too frightened to spe..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert all sentences to lowercase\n",
    "df['sentence'] = df['sentence'].str.lower()\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7bc800",
   "metadata": {},
   "source": [
    "**Sentences are a key unit of information when it comes to NLP** (as wells as tokens) in order to represent our data as a uniform \"block\" of text, we need to find out our longest sentence, the rest of them will later be padded with padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c3fdce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length is 76\n"
     ]
    }
   ],
   "source": [
    "# Get the length of the longest senctence\n",
    "max_len = df['sentence'].str.len().max()\n",
    "print(f'max sentence length is {max_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e70bd",
   "metadata": {},
   "source": [
    "The transformers library provides a convenient way to load a variety of BERT models. Let's first load and explore a tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6b2c34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fa75445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size is: 30522\n"
     ]
    }
   ],
   "source": [
    "# Get the tokenizer vocabulary words\n",
    "vocab = bert_tokenizer.vocab\n",
    "vocab_size = len(vocab)\n",
    "print(f'vocab size is: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e209cb71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7992</th>\n",
       "      <td>buddhist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24327</th>\n",
       "      <td>##metry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23429</th>\n",
       "      <td>prentice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6552</th>\n",
       "      <td>habitat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9736</th>\n",
       "      <td>poets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17274</th>\n",
       "      <td>portraying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25296</th>\n",
       "      <td>caressing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4707</th>\n",
       "      <td>alliance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8188</th>\n",
       "      <td>curled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25855</th>\n",
       "      <td>##smo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2823</th>\n",
       "      <td>sometimes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2668</th>\n",
       "      <td>blood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22623</th>\n",
       "      <td>shortages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11495</th>\n",
       "      <td>mastering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21390</th>\n",
       "      <td>seahawks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tokens\n",
       "7992     buddhist\n",
       "24327     ##metry\n",
       "23429    prentice\n",
       "6552      habitat\n",
       "9736        poets\n",
       "17274  portraying\n",
       "25296   caressing\n",
       "4707     alliance\n",
       "8188       curled\n",
       "25855       ##smo\n",
       "2823    sometimes\n",
       "2668        blood\n",
       "22623   shortages\n",
       "11495   mastering\n",
       "21390    seahawks"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the vocabulary words as a list, load them into a dataframe\n",
    "vocab_list = list(vocab.keys())\n",
    "vocab_df = pd.DataFrame(vocab_list, columns=['tokens'])\n",
    "vocab_df.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9146db8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 995 tokens that begin with \"unused\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>[unused213]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>[unused922]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>[unused765]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>[unused105]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>[unused932]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>[unused80]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>[unused943]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>[unused590]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>[unused593]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>[unused790]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tokens\n",
       "218  [unused213]\n",
       "927  [unused922]\n",
       "770  [unused765]\n",
       "110  [unused105]\n",
       "937  [unused932]\n",
       "81    [unused80]\n",
       "948  [unused943]\n",
       "595  [unused590]\n",
       "598  [unused593]\n",
       "795  [unused790]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the count of tokens that begin with 'UNUSED'\n",
    "unused_tokens = vocab_df[vocab_df['tokens'].str.find('unused')>=0]\n",
    "print(f'There are {len(unused_tokens)} tokens that begin with \"unused\"')\n",
    "unused_tokens.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8e9effe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 997 tokens that have a size of 1 character\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>ŸÇ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554</th>\n",
       "      <td>‚Çá</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1869</th>\n",
       "      <td>Êô∫</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>‡Æö</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>‡§ø</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>Œ∏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>·ÑÄ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>œâ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tokens\n",
       "1060      x\n",
       "1292      ŸÇ\n",
       "1056      t\n",
       "1554      ‚Çá\n",
       "1869      Êô∫\n",
       "1383      ‡Æö\n",
       "1341      ‡§ø\n",
       "1162      Œ∏\n",
       "1455      ·ÑÄ\n",
       "1179      œâ"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the tokens that have a size of 1 character\n",
    "one_char_tokens = vocab_df[vocab_df['tokens'].str.len()==1]\n",
    "print(f'There are {len(one_char_tokens)} tokens that have a size of 1 character')\n",
    "one_char_tokens.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ae4e532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 28042 tokens that likely reprensent English words\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4824</th>\n",
       "      <td>understanding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27661</th>\n",
       "      <td>comrade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6507</th>\n",
       "      <td>archbishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20147</th>\n",
       "      <td>##anne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24687</th>\n",
       "      <td>geometridae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6909</th>\n",
       "      <td>stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13244</th>\n",
       "      <td>meadow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8013</th>\n",
       "      <td>customer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29062</th>\n",
       "      <td>rosenthal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26014</th>\n",
       "      <td>alteration</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tokens\n",
       "4824   understanding\n",
       "27661        comrade\n",
       "6507      archbishop\n",
       "20147         ##anne\n",
       "24687    geometridae\n",
       "6909          stroke\n",
       "13244         meadow\n",
       "8013        customer\n",
       "29062      rosenthal\n",
       "26014     alteration"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the tokens which have a size of more than 2 characters and does not contain the word 'unused'\n",
    "two_char_tokens = vocab_df[(vocab_df['tokens'].str.len()>2) & (vocab_df['tokens'].str.find('unused')<0)]\n",
    "print(f'There are {len(two_char_tokens)} tokens that likely reprensent English words')\n",
    "two_char_tokens.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dc784c",
   "metadata": {},
   "source": [
    "Each sentence is currently represented as a list of characters. We need to transform this into a list of tokens, tokens then get converted into numbers using the tokenizers vocabulary as indexes. Here is an example with a phrase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "826a1502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample sentence is:\n",
      "This is a sample sentence, which we will tokenize using the BERT tokenizer.\n",
      "\n",
      "The tokenized sentence is:\n",
      "['this', 'is', 'a', 'sample', 'sentence', ',', 'which', 'we', 'will', 'token', '##ize', 'using', 'the', 'bert', 'token', '##izer', '.']\n",
      "\n",
      "The numericalized sentence is:\n",
      "[2023, 2003, 1037, 7099, 6251, 1010, 2029, 2057, 2097, 19204, 4697, 2478, 1996, 14324, 19204, 17629, 1012]\n"
     ]
    }
   ],
   "source": [
    "# Example of tokenizing a sentence\n",
    "sample_sentence = \"This is a sample sentence, which we will tokenize using the BERT tokenizer.\"\n",
    "print(f'The sample sentence is:\\n{sample_sentence}')\n",
    "\n",
    "tokenized_sentence = bert_tokenizer.tokenize(sample_sentence)\n",
    "print(f'\\nThe tokenized sentence is:\\n{tokenized_sentence}')\n",
    "\n",
    "numericalized_sentence = bert_tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "print(f'\\nThe numericalized sentence is:\\n{numericalized_sentence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79392b66",
   "metadata": {},
   "source": [
    "We should now do the same for the sentences in the dataframe. Before proceding is a good idea to create a copy of what we have so far to be able to revert back to the original dataframe in case we need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "205585c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the senctences dataframe\n",
    "tokens_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47e1b1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tokenized_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>princess would not say a word and was very sor...</td>\n",
       "      <td>[princess, would, not, say, a, word, and, was,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>but the magician beguiled him with pleasant st...</td>\n",
       "      <td>[but, the, magician, beg, ##uil, ##ed, him, wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>your answer  not so long mother as you think h...</td>\n",
       "      <td>[your, answer, not, so, long, mother, as, you,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>wife promised to me by your unjust father and ...</td>\n",
       "      <td>[wife, promised, to, me, by, your, un, ##just,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>bowl twelve silver plates containing rich meat...</td>\n",
       "      <td>[bowl, twelve, silver, plates, containing, ric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>daughter demanded the sultan  for the first i ...</td>\n",
       "      <td>[daughter, demanded, the, sultan, for, the, fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>envious vizier meanwhile hinting that it was t...</td>\n",
       "      <td>[en, ##vious, viz, ##ier, meanwhile, hint, ##i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>the bride and bridegroom  master i obey said t...</td>\n",
       "      <td>[the, bride, and, bride, ##gr, ##oom, master, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>offering to exchange fine new lamps for old on...</td>\n",
       "      <td>[offering, to, exchange, fine, new, lamps, for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>the genie appeared commanded him to bring a ro...</td>\n",
       "      <td>[the, genie, appeared, commanded, him, to, bri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  \\\n",
       "175  princess would not say a word and was very sor...   \n",
       "34   but the magician beguiled him with pleasant st...   \n",
       "206  your answer  not so long mother as you think h...   \n",
       "166  wife promised to me by your unjust father and ...   \n",
       "94   bowl twelve silver plates containing rich meat...   \n",
       "335  daughter demanded the sultan  for the first i ...   \n",
       "272  envious vizier meanwhile hinting that it was t...   \n",
       "159  the bride and bridegroom  master i obey said t...   \n",
       "302  offering to exchange fine new lamps for old on...   \n",
       "451  the genie appeared commanded him to bring a ro...   \n",
       "\n",
       "                                    tokenized_sentence  \n",
       "175  [princess, would, not, say, a, word, and, was,...  \n",
       "34   [but, the, magician, beg, ##uil, ##ed, him, wi...  \n",
       "206  [your, answer, not, so, long, mother, as, you,...  \n",
       "166  [wife, promised, to, me, by, your, un, ##just,...  \n",
       "94   [bowl, twelve, silver, plates, containing, ric...  \n",
       "335  [daughter, demanded, the, sultan, for, the, fi...  \n",
       "272  [en, ##vious, viz, ##ier, meanwhile, hint, ##i...  \n",
       "159  [the, bride, and, bride, ##gr, ##oom, master, ...  \n",
       "302  [offering, to, exchange, fine, new, lamps, for...  \n",
       "451  [the, genie, appeared, commanded, him, to, bri...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize each sentence in the dataframe\n",
    "tokens_df['tokenized_sentence'] = tokens_df['sentence'].apply(bert_tokenizer.tokenize)\n",
    "tokens_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a113239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tokenized_sentence</th>\n",
       "      <th>numericalized_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>not but he will use violence  aladdin comforte...</td>\n",
       "      <td>[not, but, he, will, use, violence, ala, ##ddi...</td>\n",
       "      <td>[2025, 2021, 2002, 2097, 2224, 4808, 21862, 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>left her arrayed herself gaily for the first t...</td>\n",
       "      <td>[left, her, array, ##ed, herself, gail, ##y, f...</td>\n",
       "      <td>[2187, 2014, 9140, 2098, 2841, 18576, 2100, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>own son begged the sultan to withhold her for ...</td>\n",
       "      <td>[own, son, begged, the, sultan, to, with, ##ho...</td>\n",
       "      <td>[2219, 2365, 12999, 1996, 7544, 2000, 2007, 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>aladdin seizing his dagger pierced him to the ...</td>\n",
       "      <td>[ala, ##ddin, seizing, his, dagger, pierced, h...</td>\n",
       "      <td>[21862, 18277, 24681, 2010, 10794, 16276, 2032...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>these halls lead into a garden of fine fruit t...</td>\n",
       "      <td>[these, halls, lead, into, a, garden, of, fine...</td>\n",
       "      <td>[2122, 9873, 2599, 2046, 1037, 3871, 1997, 298...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>wonder of the world\\r</td>\n",
       "      <td>[wonder, of, the, world]</td>\n",
       "      <td>[4687, 1997, 1996, 2088]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>lamp and kill him afterwards\\r</td>\n",
       "      <td>[lamp, and, kill, him, afterwards]</td>\n",
       "      <td>[10437, 1998, 3102, 2032, 5728]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>go to your mother and tell her i am coming  al...</td>\n",
       "      <td>[go, to, your, mother, and, tell, her, i, am, ...</td>\n",
       "      <td>[2175, 2000, 2115, 2388, 1998, 2425, 2014, 104...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>the following night exactly the same thing hap...</td>\n",
       "      <td>[the, following, night, exactly, the, same, th...</td>\n",
       "      <td>[1996, 2206, 2305, 3599, 1996, 2168, 2518, 304...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>and would fain taste those of africa  the magi...</td>\n",
       "      <td>[and, would, fai, ##n, taste, those, of, afric...</td>\n",
       "      <td>[1998, 2052, 26208, 2078, 5510, 2216, 1997, 30...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  \\\n",
       "379  not but he will use violence  aladdin comforte...   \n",
       "388  left her arrayed herself gaily for the first t...   \n",
       "143  own son begged the sultan to withhold her for ...   \n",
       "467  aladdin seizing his dagger pierced him to the ...   \n",
       "54   these halls lead into a garden of fine fruit t...   \n",
       "443                              wonder of the world\\r   \n",
       "73                      lamp and kill him afterwards\\r   \n",
       "14   go to your mother and tell her i am coming  al...   \n",
       "183  the following night exactly the same thing hap...   \n",
       "395  and would fain taste those of africa  the magi...   \n",
       "\n",
       "                                    tokenized_sentence  \\\n",
       "379  [not, but, he, will, use, violence, ala, ##ddi...   \n",
       "388  [left, her, array, ##ed, herself, gail, ##y, f...   \n",
       "143  [own, son, begged, the, sultan, to, with, ##ho...   \n",
       "467  [ala, ##ddin, seizing, his, dagger, pierced, h...   \n",
       "54   [these, halls, lead, into, a, garden, of, fine...   \n",
       "443                           [wonder, of, the, world]   \n",
       "73                  [lamp, and, kill, him, afterwards]   \n",
       "14   [go, to, your, mother, and, tell, her, i, am, ...   \n",
       "183  [the, following, night, exactly, the, same, th...   \n",
       "395  [and, would, fai, ##n, taste, those, of, afric...   \n",
       "\n",
       "                                numericalized_sentence  \n",
       "379  [2025, 2021, 2002, 2097, 2224, 4808, 21862, 18...  \n",
       "388  [2187, 2014, 9140, 2098, 2841, 18576, 2100, 20...  \n",
       "143  [2219, 2365, 12999, 1996, 7544, 2000, 2007, 12...  \n",
       "467  [21862, 18277, 24681, 2010, 10794, 16276, 2032...  \n",
       "54   [2122, 9873, 2599, 2046, 1037, 3871, 1997, 298...  \n",
       "443                           [4687, 1997, 1996, 2088]  \n",
       "73                     [10437, 1998, 3102, 2032, 5728]  \n",
       "14   [2175, 2000, 2115, 2388, 1998, 2425, 2014, 104...  \n",
       "183  [1996, 2206, 2305, 3599, 1996, 2168, 2518, 304...  \n",
       "395  [1998, 2052, 26208, 2078, 5510, 2216, 1997, 30...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the numericalized sentences to the dataframe\n",
    "tokens_df['numericalized_sentence'] = tokens_df['tokenized_sentence'].apply(bert_tokenizer.convert_tokens_to_ids)\n",
    "tokens_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8de28c0",
   "metadata": {},
   "source": [
    "Phrases that will be inputted to a BERT model must include the special tokens `[CLS]` and `[SEP]`. These tokens are used to indicate the start and end of the input sequence. Let's add these tokens to the sample phrase. Another special token is `[PAD]`, which is used to pad shorter sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c621a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The tokenized sentence is:\n",
      "['CLS', 'this', 'is', 'a', 'sample', 'sentence', ',', 'which', 'we', 'will', 'token', '##ize', 'using', 'the', 'bert', 'token', '##izer', '.', 'SEP']\n",
      "\n",
      "The numericalized sentence is:\n",
      "[100, 2023, 2003, 1037, 7099, 6251, 1010, 2029, 2057, 2097, 19204, 4697, 2478, 1996, 14324, 19204, 17629, 1012, 100]\n",
      "- The token ID for the special token [CLS] is: 101\n",
      "- The token ID for the special token [SEP] is: 102\n",
      "- The token ID for the special token [PAD] is: 0\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence = ['CLS'] + tokenized_sentence + ['SEP']\n",
    "print(f'\\nThe tokenized sentence is:\\n{tokenized_sentence}')\n",
    "\n",
    "numericalized_sentence = bert_tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "print(f'\\nThe numericalized sentence is:\\n{numericalized_sentence}')\n",
    "\n",
    "# Print the IDs for the special tokens for the BERT model\n",
    "print(f'- The token ID for the special token [CLS] is: {bert_tokenizer.cls_token_id}')\n",
    "print(f'- The token ID for the special token [SEP] is: {bert_tokenizer.sep_token_id}')\n",
    "print(f'- The token ID for the special token [PAD] is: {bert_tokenizer.pad_token_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176fd665",
   "metadata": {},
   "source": [
    "As the exampled indicates, we need to add the [CLS] and [SEP] tokens and tokenize each sentence of the text dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82dd7f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69     [101, 3894, 2808, 1997, 1037, 6919, 10437, 202...\n",
       "399    [101, 3696, 2016, 2001, 28348, 2000, 2032, 207...\n",
       "349    [101, 1045, 2572, 2069, 1996, 6658, 1997, 1996...\n",
       "350    [101, 2130, 2061, 2056, 21862, 18277, 2021, 15...\n",
       "302    [101, 5378, 2000, 3863, 2986, 2047, 14186, 200...\n",
       "311    [101, 28018, 2043, 2002, 2766, 2041, 1996, 104...\n",
       "421    [101, 2000, 24896, 2010, 3428, 2331, 1998, 225...\n",
       "134    [101, 2032, 1997, 2014, 4124, 6355, 2293, 2005...\n",
       "293    [101, 2907, 1997, 1996, 10437, 1998, 2153, 259...\n",
       "88     [101, 2210, 6557, 1998, 2097, 2175, 5271, 2009...\n",
       "Name: numericalized_sentence, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the 100 special tokens to the numericalized sentences on the dataframe\n",
    "tokens_df['numericalized_sentence'] = tokens_df['numericalized_sentence'].apply(lambda x: [bert_tokenizer.cls_token_id] + x + [bert_tokenizer.sep_token_id])\n",
    "tokens_df['numericalized_sentence'].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62b1d86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the 0 padding to the numericalized sentences on the dataframe\n",
    "tokens_df['numericalized_sentence'] = tokens_df['numericalized_sentence'].apply(lambda x: x + [bert_tokenizer.pad_token_id] * (max_len - len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1360e61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287    76\n",
       "262    76\n",
       "43     76\n",
       "36     76\n",
       "312    76\n",
       "55     76\n",
       "188    76\n",
       "133    76\n",
       "270    76\n",
       "24     76\n",
       "Name: numericalized_sentence_length, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a new column that indicates the length of the numericalized sentences\n",
    "tokens_df['numericalized_sentence_length'] = tokens_df['numericalized_sentence'].apply(len)\n",
    "tokens_df['numericalized_sentence_length'] .sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb1ed282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(447,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the numericalized sentences from the dataframe\n",
    "numericalized_sentences = tokens_df['numericalized_sentence'].values\n",
    "numericalized_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2344b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each row of the numericalized sentences to a list\n",
    "numericalized_sentences = [list(x) for x in numericalized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "feeed5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the numericalized sentences is: (447, 76)\n",
      "[[  101 21862 18277 ...     0     0     0]\n",
      " [  101  2045  2320 ...     0     0     0]\n",
      " [  101  1037 23358 ...     0     0     0]\n",
      " ...\n",
      " [  101  2044  2023 ...     0     0     0]\n",
      " [  101  2002  4594 ...     0     0     0]\n",
      " [  101  2005  2116 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# Convert the list into a 2D NumPy array\n",
    "numericalized_sentences = np.array(numericalized_sentences)\n",
    "print(f'The shape of the numericalized sentences is: {numericalized_sentences.shape}')\n",
    "print(numericalized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "465072a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101, 21862, 18277,  ...,     0,     0,     0],\n",
      "        [  101,  2045,  2320,  ...,     0,     0,     0],\n",
      "        [  101,  1037, 23358,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2044,  2023,  ...,     0,     0,     0],\n",
      "        [  101,  2002,  4594,  ...,     0,     0,     0],\n",
      "        [  101,  2005,  2116,  ...,     0,     0,     0]], dtype=torch.int32)\n",
      "the shape of the numericalized tensor is: torch.Size([447, 76])\n"
     ]
    }
   ],
   "source": [
    "#  Convert the numpy array into a Tensor\n",
    "numericalized_sentences = torch.from_numpy(numericalized_sentences)\n",
    "print(numericalized_sentences)\n",
    "print(f'the shape of the numericalized tensor is: {numericalized_sentences.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d321c2b",
   "metadata": {},
   "source": [
    "# Part 2: How to Structure Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7af91a",
   "metadata": {},
   "source": [
    "Since this problem is so diverse, it can be hard to look at. and turn it all into a 1D object, or list. \n",
    "\n",
    "If we can wrap our heads around this, we can take a soup of texts from literature, websites, reviews, almost anything imaginable, and wipe it clean so it can be tokenized. BERT recognizes this problem and doesn't rely on the text being structured in order to work. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25cb336",
   "metadata": {},
   "source": [
    "### Flattening the structure, starting from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebc42be",
   "metadata": {},
   "source": [
    "The most simple approach, or the lowest common denominator is to remove the structure of the data, and turn it into a 1D Object, either as a native Python String Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa894d79",
   "metadata": {},
   "source": [
    "### File Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d97ed0",
   "metadata": {},
   "source": [
    "## .TXT Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302f1dfa",
   "metadata": {},
   "source": [
    "The simplest method to split text data is to use the .split() method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefc0628",
   "metadata": {},
   "source": [
    "## CSV "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c0aca4",
   "metadata": {},
   "source": [
    "CSV files are organized by columns, so it makes it relatively straightforward to index the text columns. \n",
    "\n",
    "Similarly to how a numpy array can be flattened, it is not to difficult to do this and flatten it all into one string\n",
    "The concatenate method of a pandas series can do this\n",
    "\n",
    ".str.cat(sep = ' ')\n",
    ".str.cat(sep = '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00bda3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hskVocabPath = os.path.join(DATA_DIR, 'ChineseVocabulary', 'HSK Official With Definitions 2012 L3 freqorder.txt')\n",
    "os.path.exists(hskVocabPath)\n",
    "\n",
    "hskVocab = pd.read_csv(hskVocabPath,\n",
    "                       header = None,\n",
    "                       index_col = None,\n",
    "                       sep='\\t'\n",
    "                      )\n",
    "\n",
    "hskVocabColumns = ['Simplified','Traditional','Pinyin_Numeric','Pinyin_Accented','Definition']\n",
    "hskVocab.columns = hskVocabColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ee0e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "hanzi = hskVocab['Simplified'].str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60b043fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Âïä Ëøò Êää Ëøá Â¶ÇÊûú Âè™ Ë¢´ Ë∑ü Ëá™Â∑± Áî® ÂÉè ‰∏∫ ÈúÄË¶Å Â∫îËØ• Ëµ∑Êù• Êâç Âèà Êãø Êõ¥ Â∏¶ ÁÑ∂Âêé ‰∏ÄÊ†∑ ÂΩìÁÑ∂ Áõ∏‰ø° ËÆ§‰∏∫ ÊòéÁôΩ ‰∏ÄÁõ¥ Âú∞ Âú∞Êñπ Á¶ªÂºÄ ‰∏ÄÂÆö ËøòÊòØ Âèë ÂèëÁé∞ ËÄå‰∏î ÂøÖÈ°ª Êîæ ‰∏∫‰∫Ü Âêë ËÄÅ ‰Ωç ÂÖà Áßç ÊúÄÂêé ÂÖ∂‰ªñ ËÆ∞Âæó ÊàñËÄÖ ËøáÂéª ÊãÖÂøÉ Êù° ‰ª•Ââç Èïø ‰∏ñÁïå ÈáçË¶Å Âà´‰∫∫ Êú∫‰ºö Âº† Êé• ÊØîËµõ \n"
     ]
    }
   ],
   "source": [
    "print(hanzi[0:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b618a784",
   "metadata": {},
   "outputs": [],
   "source": [
    "definitions = hskVocab['Definition'].str.cat(sep='. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34bcbb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ah; (particle showing elation, doubt, puzzled surprise, or approval). still; yet; in addition; even | repay; to return. (mw for things with handles); \n"
     ]
    }
   ],
   "source": [
    "print(definitions[0:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2a62d5",
   "metadata": {},
   "source": [
    "It is also possible to join columns together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "935721e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Âïä ah; (particle showing elation, doubt, puzzle...\n",
       "1    Ëøò still; yet; in addition; even | repay; to re...\n",
       "2    Êää (mw for things with handles); (pretransitive...\n",
       "3    Ëøá to pass; to cross; go over; (indicates a pas...\n",
       "4                             Â¶ÇÊûú if; in the event that\n",
       "Name: Simplified, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hanziWithDefinitions = hskVocab['Simplified'].str.cat(\n",
    "    hskVocab['Definition'],sep=' ')\n",
    "hanziWithDefinitions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f220ff",
   "metadata": {},
   "source": [
    "It is also to concatenate rows together, if you would like to preserve a relationship when they get tokenized in the same bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51cc019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hanziWithDefinitions = hanziWithDefinitions.str.cat(sep='. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efda2a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Âïä ah; (particle showing elation, doubt, puzzled surprise, or approval). Ëøò still; yet; in addition; \n"
     ]
    }
   ],
   "source": [
    "print(hanziWithDefinitions[0:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcde2ac",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9778241f",
   "metadata": {},
   "source": [
    "Json files have a different paradigm than csv files which are table based. As a result, the results of this may vary depending on the structure. \n",
    "\n",
    "In this course, we look to cover key threshold concepts well, while being careful not to get over extended. Since tensors are dependent on unique indices and having regular shapes, this course gets into more detail about tables than it does about JSON. \n",
    "\n",
    "With this in mind, there are two things to consider about JSON files. Json files do not have to have a regular structure. One JSON cell can contain as few or as many items as required. Json files can also be heavily nested in ways that don't play well with the paradigm of tensors.\n",
    "\n",
    "If a JSON file has a regular structure, then it will fit easier into a dataframe, such as for an API request. If it does not have a regular structure, like a configuration file, then it will probably not transfer over well into a dataframe.\n",
    "\n",
    "In this case, it can be treated as a Series instead, since it is not bound by the same requirements. \n",
    "\n",
    "In many cases, it may be necessary to unpack it using python's JSON library. \n",
    "\n",
    "https://www.marsja.se/how-to-read-and-write-json-files-using-python-and-pandas/\n",
    "\n",
    "\n",
    "If you end up with some highly nested JSON objects, this tutorial goes through some pretty complicated data and might give some inspiration.\n",
    "https://medium.com/analytics-vidhya/extract-the-useful-data-from-jason-file-for-data-sceince-34ed5ae0b350\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5cb57200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sub_ID': ['1', '2', '3', '4', '5', '6', '7', '8'], 'Name': ['Erik', 'Daniel', 'Michael', 'Sven', 'Gary', 'Carol', 'Lisa', 'Elisabeth'], 'Salary': ['723.3', '515.2', '621', '731', '844.15', '558', '642.8', '732.5'], 'StartDate': ['1/1/2011', '7/23/2013', '12/15/2011', '6/11/2013', '3/27/2011', '5/21/2012', '7/30/2013', '6/17/2014'], 'Department': ['IT', 'Manegement', 'IT', 'HR', 'Finance', 'IT', 'Manegement', 'IT'], 'Sex': ['M', 'M', 'M', 'M', 'M', 'F', 'F', 'F']}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Creating a Python Dictionary\n",
    "data = {\"Sub_ID\":[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\" ],\n",
    "        \"Name\":[\"Erik\", \"Daniel\", \"Michael\", \"Sven\",\n",
    "                \"Gary\", \"Carol\",\"Lisa\", \"Elisabeth\" ],\n",
    "        \"Salary\":[\"723.3\", \"515.2\", \"621\", \"731\", \n",
    "                  \"844.15\",\"558\", \"642.8\", \"732.5\" ],\n",
    "        \"StartDate\":[ \"1/1/2011\", \"7/23/2013\", \"12/15/2011\",\n",
    "                     \"6/11/2013\", \"3/27/2011\",\"5/21/2012\", \n",
    "                     \"7/30/2013\", \"6/17/2014\"],\n",
    "        \"Department\":[ \"IT\", \"Manegement\", \"IT\", \"HR\", \n",
    "                      \"Finance\", \"IT\", \"Manegement\", \"IT\"],\n",
    "        \"Sex\":[ \"M\", \"M\", \"M\", \n",
    "              \"M\", \"M\", \"F\", \"F\", \"F\"]}\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ff1d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Parse JSON\n",
    "with open('data.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c949b4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sub_ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Salary</th>\n",
       "      <th>StartDate</th>\n",
       "      <th>Department</th>\n",
       "      <th>Sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Erik</td>\n",
       "      <td>723.30</td>\n",
       "      <td>1/1/2011</td>\n",
       "      <td>IT</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Daniel</td>\n",
       "      <td>515.20</td>\n",
       "      <td>7/23/2013</td>\n",
       "      <td>Manegement</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Michael</td>\n",
       "      <td>621.00</td>\n",
       "      <td>12/15/2011</td>\n",
       "      <td>IT</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Sven</td>\n",
       "      <td>731.00</td>\n",
       "      <td>6/11/2013</td>\n",
       "      <td>HR</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Gary</td>\n",
       "      <td>844.15</td>\n",
       "      <td>3/27/2011</td>\n",
       "      <td>Finance</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sub_ID     Name  Salary   StartDate  Department Sex\n",
       "0       1     Erik  723.30    1/1/2011          IT   M\n",
       "1       2   Daniel  515.20   7/23/2013  Manegement   M\n",
       "2       3  Michael  621.00  12/15/2011          IT   M\n",
       "3       4     Sven  731.00   6/11/2013          HR   M\n",
       "4       5     Gary  844.15   3/27/2011     Finance   M"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Read JSON as a dataframe with Pandas:\n",
    "json_df = pd.read_json('data.json')\n",
    "json_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a42561ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Erik IT. Daniel Manegement. Michael IT. Sven HR. Gary Finance. Carol IT. Lisa Manegement. Elisabeth IT'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(json_df['Name'] + ' ' + json_df['Department']).str.cat(sep='. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e385c9",
   "metadata": {},
   "source": [
    "## PDF and Word Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96e8c6f",
   "metadata": {},
   "source": [
    "PDFs and word documents contain information which makes it more readable than a plain .txt file. Word Documents are more regularly structured than a PDF, which is purely designed to be viewed by people. \n",
    "\n",
    "For this, PDF miner can be used to extract PDF documents, while python Docx can be used to extract text from a word document. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb814b5",
   "metadata": {},
   "source": [
    "# Part 3 : Discussions - Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467c9431",
   "metadata": {},
   "source": [
    "### Discuss the following:\n",
    "\n",
    "1. Why are text data sources more diverse than tabular or image data?\n",
    "\n",
    "2. Are NLP Models supervised or unsupervised?\n",
    "\n",
    "3. Do NLP Models need to have perfectly formatted data? \n",
    "\t- A. Yes, the data needs to be cleansed and put into a perfect tabular structure\n",
    "\t- B. No, because NLP recognizes this problem, and instead utilizes unsupervised learning to create it's own labels for training \n",
    "\t- C. Yes, this challenge makes it impossible to use PDFs or word documents, it must be in a .txt format to start out. \n",
    "\t- D. No, because the hyperparameters autotune the words into tokens  \n",
    "\n",
    "4. Why are tokens used instead of the words directly? \n",
    "\t- A. Saves computing resources\n",
    "\t- B. It gives a word/subword a unique identifer that the models can use in training\n",
    "\t- C. The models need to multiple, add and subtract the tokens with eachother directly\n",
    "\t- D. So that the information the words is hidden from the NLP model and it can train by making strong guesses\n",
    "\n",
    "5. What was the pipeline of this exercise?\n",
    "6. Please give a summary of the data cleaning and preprocessing steps.\n",
    "7. What is the difference between a token and a sentence?\n",
    "8. Why did we converted the tokens to numbers?\n",
    "9. Why did we add the special tokens?\n",
    "10. What advantages are offered by Pandas for text manipulation?\n",
    "11. Would this approach be suitable for complex datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d52596f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise:\n",
    "\n",
    "* Repeat this pipeline with 3 different books that appear very different in nature (don't add the special tokens).\n",
    "* When you obtain the numericalized sentences, convert them into a long 1D Numpy array.\n",
    "* Plot the distribution of the numericalized tokens for each book using histograms.\n",
    "* Comment your experience during the next lesson."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d316707d93be09212242bb4431563349bc87a6a4965cbb94d95cad94a149faa0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('DL201')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
